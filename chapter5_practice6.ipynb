{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3407f3",
   "metadata": {},
   "source": [
    "# 연습문제 6번\n",
    "\n",
    "## Improving  BP  ( Crossentropy Loss ,  Dynamic Learning Rate )\n",
    "\n",
    "## Neural Networks with a Single Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf68d9",
   "metadata": {},
   "source": [
    "### wine data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8353bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "feature_names = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', \n",
    "                 'Magnesium', 'Total phenols', 'Flavanoids', \n",
    "                'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', \n",
    "                'Hue', 'OD280', 'Proline']\n",
    "data = pd.read_csv('./data/wine/wine.txt', sep=',', \n",
    "                   names=['label'] + feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea6c07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0        1    14.23        1.71  2.43               15.6        127   \n",
       "1        1    13.20        1.78  2.14               11.2        100   \n",
       "2        1    13.16        2.36  2.67               18.6        101   \n",
       "3        1    14.37        1.95  2.50               16.8        113   \n",
       "4        1    13.24        2.59  2.87               21.0        118   \n",
       "..     ...      ...         ...   ...                ...        ...   \n",
       "173      3    13.71        5.65  2.45               20.5         95   \n",
       "174      3    13.40        3.91  2.48               23.0        102   \n",
       "175      3    13.27        4.28  2.26               20.0        120   \n",
       "176      3    13.17        2.59  2.37               20.0        120   \n",
       "177      3    14.13        4.10  2.74               24.5         96   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0             2.80        3.06                  0.28             2.29   \n",
       "1             2.65        2.76                  0.26             1.28   \n",
       "2             2.80        3.24                  0.30             2.81   \n",
       "3             3.85        3.49                  0.24             2.18   \n",
       "4             2.80        2.69                  0.39             1.82   \n",
       "..             ...         ...                   ...              ...   \n",
       "173           1.68        0.61                  0.52             1.06   \n",
       "174           1.80        0.75                  0.43             1.41   \n",
       "175           1.59        0.69                  0.43             1.35   \n",
       "176           1.65        0.68                  0.53             1.46   \n",
       "177           2.05        0.76                  0.56             1.35   \n",
       "\n",
       "     Color intensity   Hue  OD280  Proline  \n",
       "0               5.64  1.04   3.92     1065  \n",
       "1               4.38  1.05   3.40     1050  \n",
       "2               5.68  1.03   3.17     1185  \n",
       "3               7.80  0.86   3.45     1480  \n",
       "4               4.32  1.04   2.93      735  \n",
       "..               ...   ...    ...      ...  \n",
       "173             7.70  0.64   1.74      740  \n",
       "174             7.30  0.70   1.56      750  \n",
       "175            10.20  0.59   1.56      835  \n",
       "176             9.30  0.60   1.62      840  \n",
       "177             9.20  0.61   1.60      560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fa4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['label'], axis=1)\n",
    "y = pd.get_dummies(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806a5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e8f851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d116963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e092dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.721813</td>\n",
       "      <td>-0.462833</td>\n",
       "      <td>0.220227</td>\n",
       "      <td>0.281368</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.813809</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>-0.842572</td>\n",
       "      <td>0.421020</td>\n",
       "      <td>0.111933</td>\n",
       "      <td>0.435231</td>\n",
       "      <td>1.223996</td>\n",
       "      <td>0.671922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-0.261699</td>\n",
       "      <td>1.135161</td>\n",
       "      <td>0.044443</td>\n",
       "      <td>0.607325</td>\n",
       "      <td>0.438805</td>\n",
       "      <td>-0.879962</td>\n",
       "      <td>-0.819504</td>\n",
       "      <td>-1.562834</td>\n",
       "      <td>-1.291571</td>\n",
       "      <td>0.069006</td>\n",
       "      <td>-0.736839</td>\n",
       "      <td>-1.794263</td>\n",
       "      <td>-0.378672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.337354</td>\n",
       "      <td>-0.876107</td>\n",
       "      <td>-1.150887</td>\n",
       "      <td>-0.924672</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>-0.337955</td>\n",
       "      <td>-0.605445</td>\n",
       "      <td>1.318217</td>\n",
       "      <td>-1.660954</td>\n",
       "      <td>0.390955</td>\n",
       "      <td>0.164753</td>\n",
       "      <td>-1.378906</td>\n",
       "      <td>-0.871139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.677800</td>\n",
       "      <td>-1.261829</td>\n",
       "      <td>-3.471234</td>\n",
       "      <td>-2.945605</td>\n",
       "      <td>-0.774586</td>\n",
       "      <td>-0.405706</td>\n",
       "      <td>-1.461683</td>\n",
       "      <td>-0.682513</td>\n",
       "      <td>-1.996756</td>\n",
       "      <td>-1.240252</td>\n",
       "      <td>0.480311</td>\n",
       "      <td>-1.060466</td>\n",
       "      <td>-0.641321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.236481</td>\n",
       "      <td>-0.462833</td>\n",
       "      <td>-0.518065</td>\n",
       "      <td>-0.272758</td>\n",
       "      <td>-0.909407</td>\n",
       "      <td>-0.033076</td>\n",
       "      <td>0.536207</td>\n",
       "      <td>-0.842572</td>\n",
       "      <td>0.269909</td>\n",
       "      <td>-0.403185</td>\n",
       "      <td>0.976187</td>\n",
       "      <td>0.767104</td>\n",
       "      <td>-0.004398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.856635</td>\n",
       "      <td>-0.407729</td>\n",
       "      <td>0.114757</td>\n",
       "      <td>-2.489266</td>\n",
       "      <td>0.169162</td>\n",
       "      <td>1.830072</td>\n",
       "      <td>1.667665</td>\n",
       "      <td>-0.602484</td>\n",
       "      <td>2.267932</td>\n",
       "      <td>1.142169</td>\n",
       "      <td>1.156505</td>\n",
       "      <td>0.573271</td>\n",
       "      <td>2.730431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.829110</td>\n",
       "      <td>-0.536303</td>\n",
       "      <td>-0.799319</td>\n",
       "      <td>-0.207567</td>\n",
       "      <td>-1.313871</td>\n",
       "      <td>-0.964650</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.037750</td>\n",
       "      <td>0.034847</td>\n",
       "      <td>-0.617818</td>\n",
       "      <td>0.254913</td>\n",
       "      <td>0.808639</td>\n",
       "      <td>-0.674152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.419194</td>\n",
       "      <td>1.530068</td>\n",
       "      <td>-0.201654</td>\n",
       "      <td>-0.696502</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.712183</td>\n",
       "      <td>0.638140</td>\n",
       "      <td>-0.362396</td>\n",
       "      <td>0.085218</td>\n",
       "      <td>-0.205723</td>\n",
       "      <td>-0.556521</td>\n",
       "      <td>0.573271</td>\n",
       "      <td>-0.116024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-1.043465</td>\n",
       "      <td>-1.087336</td>\n",
       "      <td>0.571795</td>\n",
       "      <td>1.422218</td>\n",
       "      <td>-1.448692</td>\n",
       "      <td>-0.371831</td>\n",
       "      <td>-0.432158</td>\n",
       "      <td>0.277837</td>\n",
       "      <td>-0.351325</td>\n",
       "      <td>-1.132936</td>\n",
       "      <td>1.652381</td>\n",
       "      <td>0.185604</td>\n",
       "      <td>-0.280179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-0.413009</td>\n",
       "      <td>-0.940394</td>\n",
       "      <td>-0.869633</td>\n",
       "      <td>0.118390</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>-1.252592</td>\n",
       "      <td>-1.451490</td>\n",
       "      <td>1.318217</td>\n",
       "      <td>-0.351325</td>\n",
       "      <td>1.185095</td>\n",
       "      <td>-1.638432</td>\n",
       "      <td>-1.434287</td>\n",
       "      <td>-0.247348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "35   0.721813   -0.462833  0.220227           0.281368   0.034341   \n",
       "133 -0.261699    1.135161  0.044443           0.607325   0.438805   \n",
       "61  -0.337354   -0.876107 -1.150887          -0.924672   0.034341   \n",
       "59  -0.677800   -1.261829 -3.471234          -2.945605  -0.774586   \n",
       "81  -0.236481   -0.462833 -0.518065          -0.272758  -0.909407   \n",
       "..        ...         ...       ...                ...        ...   \n",
       "14   1.856635   -0.407729  0.114757          -2.489266   0.169162   \n",
       "106 -0.829110   -0.536303 -0.799319          -0.207567  -1.313871   \n",
       "43   0.419194    1.530068 -0.201654          -0.696502   0.236573   \n",
       "82  -1.043465   -1.087336  0.571795           1.422218  -1.448692   \n",
       "154 -0.413009   -0.940394 -0.869633           0.118390   0.236573   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "35        0.813809    0.994906             -0.842572         0.421020   \n",
       "133      -0.879962   -0.819504             -1.562834        -1.291571   \n",
       "61       -0.337955   -0.605445              1.318217        -1.660954   \n",
       "59       -0.405706   -1.461683             -0.682513        -1.996756   \n",
       "81       -0.033076    0.536207             -0.842572         0.269909   \n",
       "..             ...         ...                   ...              ...   \n",
       "14        1.830072    1.667665             -0.602484         2.267932   \n",
       "106      -0.964650    0.026541              0.037750         0.034847   \n",
       "43        0.712183    0.638140             -0.362396         0.085218   \n",
       "82       -0.371831   -0.432158              0.277837        -0.351325   \n",
       "154      -1.252592   -1.451490              1.318217        -0.351325   \n",
       "\n",
       "     Color intensity       Hue     OD280   Proline  \n",
       "35          0.111933  0.435231  1.223996  0.671922  \n",
       "133         0.069006 -0.736839 -1.794263 -0.378672  \n",
       "61          0.390955  0.164753 -1.378906 -0.871139  \n",
       "59         -1.240252  0.480311 -1.060466 -0.641321  \n",
       "81         -0.403185  0.976187  0.767104 -0.004398  \n",
       "..               ...       ...       ...       ...  \n",
       "14          1.142169  1.156505  0.573271  2.730431  \n",
       "106        -0.617818  0.254913  0.808639 -0.674152  \n",
       "43         -0.205723 -0.556521  0.573271 -0.116024  \n",
       "82         -1.132936  1.652381  0.185604 -0.280179  \n",
       "154         1.185095 -1.638432 -1.434287 -0.247348  \n",
       "\n",
       "[106 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d82e90",
   "metadata": {},
   "source": [
    "### 표준 BP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54390936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBP:\n",
    "    def __init__(self, lr=0.1, epochs=100, n_hidden_layer_node=20):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_hidden_layer_node = n_hidden_layer_node\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_exp = np.exp(x)\n",
    "        return x_exp / np.sum(x_exp, axis=1)[..., np.newaxis]\n",
    "    \n",
    "    def update(self, x_, y_):\n",
    "        x2 = self.sigmoid(np.dot(x_, self.w1) + self.b1) # 1 x n_hidden_layer_node\n",
    "        y2 = self.softmax(np.dot(x2, self.w2) + self.b2) # 1 x n_output_node\n",
    "        \n",
    "        y2y_diff = y2 - y_\n",
    "\n",
    "        grad_b1 = np.dot(y2y_diff, self.w2.T) * x2 * (1 - x2)\n",
    "        grad_w1 = np.dot(x_.T, grad_b1)\n",
    "        grad_b2 = y2y_diff\n",
    "        grad_w2 = np.dot(x2.T , y2y_diff)\n",
    "        \n",
    "        self.b1 -= self.lr * grad_b1\n",
    "        self.w1 -= self.lr * grad_w1\n",
    "        self.b2 -= self.lr * grad_b2\n",
    "        self.w2 -= self.lr * grad_w2\n",
    "        \n",
    "    def crossentropy(self, y_hat, y_):\n",
    "        return np.mean(-np.sum(y_ * np.log(y_hat), axis=1))\n",
    "    \n",
    "    def accuracy(self, y_hat, y_):\n",
    "        return np.mean((np.argmax(y_hat, axis=1) == np.argmax(y_, axis=1)))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x, y = x.values, y.values\n",
    "        n_input_node = x.shape[-1]\n",
    "        n_output_node = y.shape[-1]\n",
    "        \n",
    "        self.length = len(x)\n",
    "        \n",
    "        self.w1 = np.random.randn(n_input_node, self.n_hidden_layer_node)\n",
    "        self.b1 = np.zeros((1, self.n_hidden_layer_node))\n",
    "        \n",
    "        self.w2 = np.random.randn(self.n_hidden_layer_node, n_output_node)\n",
    "        self.b2 = np.zeros((1, n_output_node))\n",
    "        \n",
    "        loss_lst = []\n",
    "        acc_lst = []\n",
    "        \n",
    "        for j in range(self.epochs):\n",
    "            for i in range(self.length):\n",
    "                x_, y_ = x[i:i+1], y[i:i+1]  # 1x19  1x1\n",
    "                \n",
    "                self.update(x_, y_)\n",
    "\n",
    "            pred = self.predict(x)\n",
    "            loss = self.crossentropy(pred, y)\n",
    "            acc = self.accuracy(pred, y)\n",
    "            \n",
    "            loss_lst += [loss]\n",
    "            acc_lst += [acc]\n",
    "            \n",
    "            print(\"EPOCHS: {} | LOSS: {:.5f} | ACC: {:.4f}\".format(j, loss, acc))\n",
    "            \n",
    "        return loss_lst, acc_lst\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.softmax(np.dot(self.sigmoid(np.dot(x, self.w1) + self.b1), self.w2) + self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201b94d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 0 | LOSS: 0.17091 | ACC: 0.9623\n",
      "EPOCHS: 1 | LOSS: 0.09096 | ACC: 0.9811\n",
      "EPOCHS: 2 | LOSS: 0.05926 | ACC: 0.9811\n",
      "EPOCHS: 3 | LOSS: 0.04224 | ACC: 0.9906\n",
      "EPOCHS: 4 | LOSS: 0.03209 | ACC: 1.0000\n",
      "EPOCHS: 5 | LOSS: 0.02562 | ACC: 1.0000\n",
      "EPOCHS: 6 | LOSS: 0.02126 | ACC: 1.0000\n",
      "EPOCHS: 7 | LOSS: 0.01821 | ACC: 1.0000\n",
      "EPOCHS: 8 | LOSS: 0.01598 | ACC: 1.0000\n",
      "EPOCHS: 9 | LOSS: 0.01426 | ACC: 1.0000\n",
      "EPOCHS: 10 | LOSS: 0.01289 | ACC: 1.0000\n",
      "EPOCHS: 11 | LOSS: 0.01176 | ACC: 1.0000\n",
      "EPOCHS: 12 | LOSS: 0.01082 | ACC: 1.0000\n",
      "EPOCHS: 13 | LOSS: 0.01001 | ACC: 1.0000\n",
      "EPOCHS: 14 | LOSS: 0.00932 | ACC: 1.0000\n",
      "EPOCHS: 15 | LOSS: 0.00872 | ACC: 1.0000\n",
      "EPOCHS: 16 | LOSS: 0.00819 | ACC: 1.0000\n",
      "EPOCHS: 17 | LOSS: 0.00773 | ACC: 1.0000\n",
      "EPOCHS: 18 | LOSS: 0.00731 | ACC: 1.0000\n",
      "EPOCHS: 19 | LOSS: 0.00693 | ACC: 1.0000\n",
      "EPOCHS: 20 | LOSS: 0.00660 | ACC: 1.0000\n",
      "EPOCHS: 21 | LOSS: 0.00629 | ACC: 1.0000\n",
      "EPOCHS: 22 | LOSS: 0.00601 | ACC: 1.0000\n",
      "EPOCHS: 23 | LOSS: 0.00575 | ACC: 1.0000\n",
      "EPOCHS: 24 | LOSS: 0.00552 | ACC: 1.0000\n",
      "EPOCHS: 25 | LOSS: 0.00530 | ACC: 1.0000\n",
      "EPOCHS: 26 | LOSS: 0.00510 | ACC: 1.0000\n",
      "EPOCHS: 27 | LOSS: 0.00492 | ACC: 1.0000\n",
      "EPOCHS: 28 | LOSS: 0.00474 | ACC: 1.0000\n",
      "EPOCHS: 29 | LOSS: 0.00458 | ACC: 1.0000\n",
      "EPOCHS: 30 | LOSS: 0.00443 | ACC: 1.0000\n",
      "EPOCHS: 31 | LOSS: 0.00429 | ACC: 1.0000\n",
      "EPOCHS: 32 | LOSS: 0.00415 | ACC: 1.0000\n",
      "EPOCHS: 33 | LOSS: 0.00403 | ACC: 1.0000\n",
      "EPOCHS: 34 | LOSS: 0.00391 | ACC: 1.0000\n",
      "EPOCHS: 35 | LOSS: 0.00380 | ACC: 1.0000\n",
      "EPOCHS: 36 | LOSS: 0.00369 | ACC: 1.0000\n",
      "EPOCHS: 37 | LOSS: 0.00359 | ACC: 1.0000\n",
      "EPOCHS: 38 | LOSS: 0.00350 | ACC: 1.0000\n",
      "EPOCHS: 39 | LOSS: 0.00341 | ACC: 1.0000\n",
      "EPOCHS: 40 | LOSS: 0.00332 | ACC: 1.0000\n",
      "EPOCHS: 41 | LOSS: 0.00324 | ACC: 1.0000\n",
      "EPOCHS: 42 | LOSS: 0.00317 | ACC: 1.0000\n",
      "EPOCHS: 43 | LOSS: 0.00309 | ACC: 1.0000\n",
      "EPOCHS: 44 | LOSS: 0.00302 | ACC: 1.0000\n",
      "EPOCHS: 45 | LOSS: 0.00295 | ACC: 1.0000\n",
      "EPOCHS: 46 | LOSS: 0.00289 | ACC: 1.0000\n",
      "EPOCHS: 47 | LOSS: 0.00283 | ACC: 1.0000\n",
      "EPOCHS: 48 | LOSS: 0.00277 | ACC: 1.0000\n",
      "EPOCHS: 49 | LOSS: 0.00271 | ACC: 1.0000\n",
      "EPOCHS: 50 | LOSS: 0.00265 | ACC: 1.0000\n",
      "EPOCHS: 51 | LOSS: 0.00260 | ACC: 1.0000\n",
      "EPOCHS: 52 | LOSS: 0.00255 | ACC: 1.0000\n",
      "EPOCHS: 53 | LOSS: 0.00250 | ACC: 1.0000\n",
      "EPOCHS: 54 | LOSS: 0.00246 | ACC: 1.0000\n",
      "EPOCHS: 55 | LOSS: 0.00241 | ACC: 1.0000\n",
      "EPOCHS: 56 | LOSS: 0.00237 | ACC: 1.0000\n",
      "EPOCHS: 57 | LOSS: 0.00232 | ACC: 1.0000\n",
      "EPOCHS: 58 | LOSS: 0.00228 | ACC: 1.0000\n",
      "EPOCHS: 59 | LOSS: 0.00224 | ACC: 1.0000\n",
      "EPOCHS: 60 | LOSS: 0.00221 | ACC: 1.0000\n",
      "EPOCHS: 61 | LOSS: 0.00217 | ACC: 1.0000\n",
      "EPOCHS: 62 | LOSS: 0.00213 | ACC: 1.0000\n",
      "EPOCHS: 63 | LOSS: 0.00210 | ACC: 1.0000\n",
      "EPOCHS: 64 | LOSS: 0.00206 | ACC: 1.0000\n",
      "EPOCHS: 65 | LOSS: 0.00203 | ACC: 1.0000\n",
      "EPOCHS: 66 | LOSS: 0.00200 | ACC: 1.0000\n",
      "EPOCHS: 67 | LOSS: 0.00197 | ACC: 1.0000\n",
      "EPOCHS: 68 | LOSS: 0.00194 | ACC: 1.0000\n",
      "EPOCHS: 69 | LOSS: 0.00191 | ACC: 1.0000\n",
      "EPOCHS: 70 | LOSS: 0.00188 | ACC: 1.0000\n",
      "EPOCHS: 71 | LOSS: 0.00186 | ACC: 1.0000\n",
      "EPOCHS: 72 | LOSS: 0.00183 | ACC: 1.0000\n",
      "EPOCHS: 73 | LOSS: 0.00180 | ACC: 1.0000\n",
      "EPOCHS: 74 | LOSS: 0.00178 | ACC: 1.0000\n",
      "EPOCHS: 75 | LOSS: 0.00175 | ACC: 1.0000\n",
      "EPOCHS: 76 | LOSS: 0.00173 | ACC: 1.0000\n",
      "EPOCHS: 77 | LOSS: 0.00171 | ACC: 1.0000\n",
      "EPOCHS: 78 | LOSS: 0.00168 | ACC: 1.0000\n",
      "EPOCHS: 79 | LOSS: 0.00166 | ACC: 1.0000\n",
      "EPOCHS: 80 | LOSS: 0.00164 | ACC: 1.0000\n",
      "EPOCHS: 81 | LOSS: 0.00162 | ACC: 1.0000\n",
      "EPOCHS: 82 | LOSS: 0.00160 | ACC: 1.0000\n",
      "EPOCHS: 83 | LOSS: 0.00158 | ACC: 1.0000\n",
      "EPOCHS: 84 | LOSS: 0.00156 | ACC: 1.0000\n",
      "EPOCHS: 85 | LOSS: 0.00154 | ACC: 1.0000\n",
      "EPOCHS: 86 | LOSS: 0.00152 | ACC: 1.0000\n",
      "EPOCHS: 87 | LOSS: 0.00150 | ACC: 1.0000\n",
      "EPOCHS: 88 | LOSS: 0.00149 | ACC: 1.0000\n",
      "EPOCHS: 89 | LOSS: 0.00147 | ACC: 1.0000\n",
      "EPOCHS: 90 | LOSS: 0.00145 | ACC: 1.0000\n",
      "EPOCHS: 91 | LOSS: 0.00144 | ACC: 1.0000\n",
      "EPOCHS: 92 | LOSS: 0.00142 | ACC: 1.0000\n",
      "EPOCHS: 93 | LOSS: 0.00140 | ACC: 1.0000\n",
      "EPOCHS: 94 | LOSS: 0.00139 | ACC: 1.0000\n",
      "EPOCHS: 95 | LOSS: 0.00137 | ACC: 1.0000\n",
      "EPOCHS: 96 | LOSS: 0.00136 | ACC: 1.0000\n",
      "EPOCHS: 97 | LOSS: 0.00134 | ACC: 1.0000\n",
      "EPOCHS: 98 | LOSS: 0.00133 | ACC: 1.0000\n",
      "EPOCHS: 99 | LOSS: 0.00132 | ACC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = NNBP(epochs=100, lr=0.1, n_hidden_layer_node=20)\n",
    "loss_lst1, acc_lst1 = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d4da3",
   "metadata": {},
   "source": [
    "### Momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafaa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBP:\n",
    "    def __init__(self, lr=0.1, epochs=100, n_hidden_layer_node=20, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_hidden_layer_node = n_hidden_layer_node\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_exp = np.exp(x)\n",
    "        return x_exp / np.sum(x_exp, axis=1)[..., np.newaxis]\n",
    "    \n",
    "    def update(self, x_, y_):\n",
    "        x2 = self.sigmoid(np.dot(x_, self.w1) + self.b1) # 1 x n_hidden_layer_node\n",
    "        y2 = self.softmax(np.dot(x2, self.w2) + self.b2) # 1 x n_output_node\n",
    "        \n",
    "        y2y_diff = y2 - y_\n",
    "\n",
    "        grad_b1 = np.dot(y2y_diff, self.w2.T) * x2 * (1 - x2)\n",
    "        grad_w1 = np.dot(x_.T, grad_b1)\n",
    "        grad_b2 = y2y_diff\n",
    "        grad_w2 = np.dot(x2.T , y2y_diff)\n",
    "        \n",
    "        b1_v, w1_v, b2_v, w2_v = 0, 0, 0, 0\n",
    "        b1_v = self.momentum * b1_v - self.lr * grad_b1\n",
    "        b2_v = self.momentum * b2_v - self.lr * grad_b2\n",
    "        w1_v = self.momentum * w1_v - self.lr * grad_w1\n",
    "        w2_v = self.momentum * w2_v - self.lr * grad_w2\n",
    "        \n",
    "        self.b1 += b1_v\n",
    "        self.w1 += w1_v\n",
    "        self.b2 += b2_v\n",
    "        self.w2 += w2_v\n",
    "        \n",
    "    def crossentropy(self, y_hat, y_):\n",
    "        return np.mean(-np.sum(y_ * np.log(y_hat), axis=1))\n",
    "    \n",
    "    def accuracy(self, y_hat, y_):\n",
    "        return np.mean((np.argmax(y_hat, axis=1) == np.argmax(y_, axis=1)))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x, y = x.values, y.values\n",
    "        n_input_node = x.shape[-1]\n",
    "        n_output_node = y.shape[-1]\n",
    "        \n",
    "        self.length = len(x)\n",
    "        \n",
    "        self.w1 = np.random.randn(n_input_node, self.n_hidden_layer_node)\n",
    "        self.b1 = np.zeros((1, self.n_hidden_layer_node))\n",
    "        \n",
    "        self.w2 = np.random.randn(self.n_hidden_layer_node, n_output_node)\n",
    "        self.b2 = np.zeros((1, n_output_node))\n",
    "        \n",
    "        loss_lst = []\n",
    "        acc_lst = []\n",
    "        \n",
    "        for j in range(self.epochs):\n",
    "            for i in range(self.length):\n",
    "                x_, y_ = x[i:i+1], y[i:i+1]  # 1x19  1x1\n",
    "                \n",
    "                self.update(x_, y_)\n",
    "\n",
    "            pred = self.predict(x)\n",
    "            loss = self.crossentropy(pred, y)\n",
    "            acc = self.accuracy(pred, y)\n",
    "            \n",
    "            loss_lst += [loss]\n",
    "            acc_lst += [acc]\n",
    "            \n",
    "            print(\"EPOCHS: {} | LOSS: {:.5f} | ACC: {:.4f}\".format(j, loss, acc))\n",
    "            \n",
    "        return loss_lst, acc_lst\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.softmax(np.dot(self.sigmoid(np.dot(x, self.w1) + self.b1), self.w2) + self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04889b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 0 | LOSS: 0.18946 | ACC: 0.9340\n",
      "EPOCHS: 1 | LOSS: 0.10777 | ACC: 0.9717\n",
      "EPOCHS: 2 | LOSS: 0.07150 | ACC: 0.9906\n",
      "EPOCHS: 3 | LOSS: 0.05256 | ACC: 1.0000\n",
      "EPOCHS: 4 | LOSS: 0.04117 | ACC: 1.0000\n",
      "EPOCHS: 5 | LOSS: 0.03360 | ACC: 1.0000\n",
      "EPOCHS: 6 | LOSS: 0.02828 | ACC: 1.0000\n",
      "EPOCHS: 7 | LOSS: 0.02438 | ACC: 1.0000\n",
      "EPOCHS: 8 | LOSS: 0.02141 | ACC: 1.0000\n",
      "EPOCHS: 9 | LOSS: 0.01908 | ACC: 1.0000\n",
      "EPOCHS: 10 | LOSS: 0.01721 | ACC: 1.0000\n",
      "EPOCHS: 11 | LOSS: 0.01567 | ACC: 1.0000\n",
      "EPOCHS: 12 | LOSS: 0.01437 | ACC: 1.0000\n",
      "EPOCHS: 13 | LOSS: 0.01328 | ACC: 1.0000\n",
      "EPOCHS: 14 | LOSS: 0.01233 | ACC: 1.0000\n",
      "EPOCHS: 15 | LOSS: 0.01151 | ACC: 1.0000\n",
      "EPOCHS: 16 | LOSS: 0.01079 | ACC: 1.0000\n",
      "EPOCHS: 17 | LOSS: 0.01015 | ACC: 1.0000\n",
      "EPOCHS: 18 | LOSS: 0.00958 | ACC: 1.0000\n",
      "EPOCHS: 19 | LOSS: 0.00907 | ACC: 1.0000\n",
      "EPOCHS: 20 | LOSS: 0.00861 | ACC: 1.0000\n",
      "EPOCHS: 21 | LOSS: 0.00819 | ACC: 1.0000\n",
      "EPOCHS: 22 | LOSS: 0.00781 | ACC: 1.0000\n",
      "EPOCHS: 23 | LOSS: 0.00746 | ACC: 1.0000\n",
      "EPOCHS: 24 | LOSS: 0.00714 | ACC: 1.0000\n",
      "EPOCHS: 25 | LOSS: 0.00684 | ACC: 1.0000\n",
      "EPOCHS: 26 | LOSS: 0.00657 | ACC: 1.0000\n",
      "EPOCHS: 27 | LOSS: 0.00632 | ACC: 1.0000\n",
      "EPOCHS: 28 | LOSS: 0.00609 | ACC: 1.0000\n",
      "EPOCHS: 29 | LOSS: 0.00587 | ACC: 1.0000\n",
      "EPOCHS: 30 | LOSS: 0.00566 | ACC: 1.0000\n",
      "EPOCHS: 31 | LOSS: 0.00547 | ACC: 1.0000\n",
      "EPOCHS: 32 | LOSS: 0.00529 | ACC: 1.0000\n",
      "EPOCHS: 33 | LOSS: 0.00513 | ACC: 1.0000\n",
      "EPOCHS: 34 | LOSS: 0.00497 | ACC: 1.0000\n",
      "EPOCHS: 35 | LOSS: 0.00482 | ACC: 1.0000\n",
      "EPOCHS: 36 | LOSS: 0.00468 | ACC: 1.0000\n",
      "EPOCHS: 37 | LOSS: 0.00454 | ACC: 1.0000\n",
      "EPOCHS: 38 | LOSS: 0.00442 | ACC: 1.0000\n",
      "EPOCHS: 39 | LOSS: 0.00430 | ACC: 1.0000\n",
      "EPOCHS: 40 | LOSS: 0.00418 | ACC: 1.0000\n",
      "EPOCHS: 41 | LOSS: 0.00408 | ACC: 1.0000\n",
      "EPOCHS: 42 | LOSS: 0.00397 | ACC: 1.0000\n",
      "EPOCHS: 43 | LOSS: 0.00387 | ACC: 1.0000\n",
      "EPOCHS: 44 | LOSS: 0.00378 | ACC: 1.0000\n",
      "EPOCHS: 45 | LOSS: 0.00369 | ACC: 1.0000\n",
      "EPOCHS: 46 | LOSS: 0.00361 | ACC: 1.0000\n",
      "EPOCHS: 47 | LOSS: 0.00352 | ACC: 1.0000\n",
      "EPOCHS: 48 | LOSS: 0.00344 | ACC: 1.0000\n",
      "EPOCHS: 49 | LOSS: 0.00337 | ACC: 1.0000\n",
      "EPOCHS: 50 | LOSS: 0.00330 | ACC: 1.0000\n",
      "EPOCHS: 51 | LOSS: 0.00323 | ACC: 1.0000\n",
      "EPOCHS: 52 | LOSS: 0.00316 | ACC: 1.0000\n",
      "EPOCHS: 53 | LOSS: 0.00310 | ACC: 1.0000\n",
      "EPOCHS: 54 | LOSS: 0.00303 | ACC: 1.0000\n",
      "EPOCHS: 55 | LOSS: 0.00298 | ACC: 1.0000\n",
      "EPOCHS: 56 | LOSS: 0.00292 | ACC: 1.0000\n",
      "EPOCHS: 57 | LOSS: 0.00286 | ACC: 1.0000\n",
      "EPOCHS: 58 | LOSS: 0.00281 | ACC: 1.0000\n",
      "EPOCHS: 59 | LOSS: 0.00276 | ACC: 1.0000\n",
      "EPOCHS: 60 | LOSS: 0.00271 | ACC: 1.0000\n",
      "EPOCHS: 61 | LOSS: 0.00266 | ACC: 1.0000\n",
      "EPOCHS: 62 | LOSS: 0.00261 | ACC: 1.0000\n",
      "EPOCHS: 63 | LOSS: 0.00257 | ACC: 1.0000\n",
      "EPOCHS: 64 | LOSS: 0.00252 | ACC: 1.0000\n",
      "EPOCHS: 65 | LOSS: 0.00248 | ACC: 1.0000\n",
      "EPOCHS: 66 | LOSS: 0.00244 | ACC: 1.0000\n",
      "EPOCHS: 67 | LOSS: 0.00240 | ACC: 1.0000\n",
      "EPOCHS: 68 | LOSS: 0.00236 | ACC: 1.0000\n",
      "EPOCHS: 69 | LOSS: 0.00232 | ACC: 1.0000\n",
      "EPOCHS: 70 | LOSS: 0.00229 | ACC: 1.0000\n",
      "EPOCHS: 71 | LOSS: 0.00225 | ACC: 1.0000\n",
      "EPOCHS: 72 | LOSS: 0.00222 | ACC: 1.0000\n",
      "EPOCHS: 73 | LOSS: 0.00218 | ACC: 1.0000\n",
      "EPOCHS: 74 | LOSS: 0.00215 | ACC: 1.0000\n",
      "EPOCHS: 75 | LOSS: 0.00212 | ACC: 1.0000\n",
      "EPOCHS: 76 | LOSS: 0.00209 | ACC: 1.0000\n",
      "EPOCHS: 77 | LOSS: 0.00206 | ACC: 1.0000\n",
      "EPOCHS: 78 | LOSS: 0.00203 | ACC: 1.0000\n",
      "EPOCHS: 79 | LOSS: 0.00200 | ACC: 1.0000\n",
      "EPOCHS: 80 | LOSS: 0.00197 | ACC: 1.0000\n",
      "EPOCHS: 81 | LOSS: 0.00195 | ACC: 1.0000\n",
      "EPOCHS: 82 | LOSS: 0.00192 | ACC: 1.0000\n",
      "EPOCHS: 83 | LOSS: 0.00190 | ACC: 1.0000\n",
      "EPOCHS: 84 | LOSS: 0.00187 | ACC: 1.0000\n",
      "EPOCHS: 85 | LOSS: 0.00185 | ACC: 1.0000\n",
      "EPOCHS: 86 | LOSS: 0.00182 | ACC: 1.0000\n",
      "EPOCHS: 87 | LOSS: 0.00180 | ACC: 1.0000\n",
      "EPOCHS: 88 | LOSS: 0.00178 | ACC: 1.0000\n",
      "EPOCHS: 89 | LOSS: 0.00175 | ACC: 1.0000\n",
      "EPOCHS: 90 | LOSS: 0.00173 | ACC: 1.0000\n",
      "EPOCHS: 91 | LOSS: 0.00171 | ACC: 1.0000\n",
      "EPOCHS: 92 | LOSS: 0.00169 | ACC: 1.0000\n",
      "EPOCHS: 93 | LOSS: 0.00167 | ACC: 1.0000\n",
      "EPOCHS: 94 | LOSS: 0.00165 | ACC: 1.0000\n",
      "EPOCHS: 95 | LOSS: 0.00163 | ACC: 1.0000\n",
      "EPOCHS: 96 | LOSS: 0.00161 | ACC: 1.0000\n",
      "EPOCHS: 97 | LOSS: 0.00159 | ACC: 1.0000\n",
      "EPOCHS: 98 | LOSS: 0.00157 | ACC: 1.0000\n",
      "EPOCHS: 99 | LOSS: 0.00156 | ACC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = NNBP(epochs=100, lr=0.1, n_hidden_layer_node=20)\n",
    "loss_lst2, acc_lst2 = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53562e1f",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9fbf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBP:\n",
    "    def __init__(self, lr=0.1, epochs=100, n_hidden_layer_node=20, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_hidden_layer_node = n_hidden_layer_node\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_exp = np.exp(x)\n",
    "        return x_exp / np.sum(x_exp, axis=1)[..., np.newaxis]\n",
    "    \n",
    "    def update(self, x_, y_):\n",
    "        x2 = self.sigmoid(np.dot(x_, self.w1) + self.b1) # 1 x n_hidden_layer_node\n",
    "        y2 = self.softmax(np.dot(x2, self.w2) + self.b2) # 1 x n_output_node\n",
    "        \n",
    "        y2y_diff = y2 - y_\n",
    "\n",
    "        grad_b1 = np.dot(y2y_diff, self.w2.T) * x2 * (1 - x2)\n",
    "        grad_w1 = np.dot(x_.T, grad_b1)\n",
    "        grad_b2 = y2y_diff\n",
    "        grad_w2 = np.dot(x2.T , y2y_diff)\n",
    "        \n",
    "        b1_h, w1_h, b2_h, w2_h = 0, 0, 0, 0\n",
    "        b1_h += grad_b1 * grad_b1\n",
    "        b2_h += grad_b2 * grad_b2\n",
    "        w1_h += grad_w1 * grad_w1\n",
    "        w2_h += grad_w2 * grad_w2\n",
    "        \n",
    "        self.b1 -= self.lr / (np.sqrt(b1_h) + 1e-7) * grad_b1\n",
    "        self.w1 -= self.lr / (np.sqrt(w1_h) + 1e-7) * grad_w1\n",
    "        self.b2 -= self.lr / (np.sqrt(b2_h) + 1e-7) * grad_b2\n",
    "        self.w2 -= self.lr / (np.sqrt(w2_h) + 1e-7) * grad_w2\n",
    "        \n",
    "    def crossentropy(self, y_hat, y_):\n",
    "        return np.mean(-np.sum(y_ * np.log(y_hat), axis=1))\n",
    "    \n",
    "    def accuracy(self, y_hat, y_):\n",
    "        return np.mean((np.argmax(y_hat, axis=1) == np.argmax(y_, axis=1)))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x, y = x.values, y.values\n",
    "        n_input_node = x.shape[-1]\n",
    "        n_output_node = y.shape[-1]\n",
    "        \n",
    "        self.length = len(x)\n",
    "        \n",
    "        self.w1 = np.random.randn(n_input_node, self.n_hidden_layer_node)\n",
    "        self.b1 = np.zeros((1, self.n_hidden_layer_node))\n",
    "        \n",
    "        self.w2 = np.random.randn(self.n_hidden_layer_node, n_output_node)\n",
    "        self.b2 = np.zeros((1, n_output_node))\n",
    "        \n",
    "        loss_lst = []\n",
    "        acc_lst = []\n",
    "        \n",
    "        for j in range(self.epochs):\n",
    "            for i in range(self.length):\n",
    "                x_, y_ = x[i:i+1], y[i:i+1]  # 1x19  1x1\n",
    "                \n",
    "                self.update(x_, y_)\n",
    "\n",
    "            pred = self.predict(x)\n",
    "            loss = self.crossentropy(pred, y)\n",
    "            acc = self.accuracy(pred, y)\n",
    "            \n",
    "            loss_lst += [loss]\n",
    "            acc_lst += [acc]\n",
    "            \n",
    "            print(\"EPOCHS: {} | LOSS: {:.5f} | ACC: {:.4f}\".format(j, loss, acc))\n",
    "            \n",
    "        return loss_lst, acc_lst\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.softmax(np.dot(self.sigmoid(np.dot(x, self.w1) + self.b1), self.w2) + self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f7d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 0 | LOSS: 0.20543 | ACC: 0.9528\n",
      "EPOCHS: 1 | LOSS: 0.17820 | ACC: 0.9717\n",
      "EPOCHS: 2 | LOSS: 0.18678 | ACC: 0.9717\n",
      "EPOCHS: 3 | LOSS: 0.15629 | ACC: 0.9717\n",
      "EPOCHS: 4 | LOSS: 0.10751 | ACC: 0.9811\n",
      "EPOCHS: 5 | LOSS: 0.08574 | ACC: 0.9906\n",
      "EPOCHS: 6 | LOSS: 0.06987 | ACC: 0.9906\n",
      "EPOCHS: 7 | LOSS: 0.05294 | ACC: 0.9906\n",
      "EPOCHS: 8 | LOSS: 0.03187 | ACC: 0.9906\n",
      "EPOCHS: 9 | LOSS: 0.01471 | ACC: 0.9906\n",
      "EPOCHS: 10 | LOSS: 0.00937 | ACC: 0.9906\n",
      "EPOCHS: 11 | LOSS: 0.00838 | ACC: 0.9906\n",
      "EPOCHS: 12 | LOSS: 0.01095 | ACC: 0.9906\n",
      "EPOCHS: 13 | LOSS: 0.01405 | ACC: 0.9906\n",
      "EPOCHS: 14 | LOSS: 0.01011 | ACC: 0.9906\n",
      "EPOCHS: 15 | LOSS: 0.00854 | ACC: 0.9906\n",
      "EPOCHS: 16 | LOSS: 0.00256 | ACC: 1.0000\n",
      "EPOCHS: 17 | LOSS: 0.00959 | ACC: 0.9906\n",
      "EPOCHS: 18 | LOSS: 0.00928 | ACC: 0.9906\n",
      "EPOCHS: 19 | LOSS: 0.00041 | ACC: 1.0000\n",
      "EPOCHS: 20 | LOSS: 0.00001 | ACC: 1.0000\n",
      "EPOCHS: 21 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 22 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 23 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 24 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 25 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 26 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 27 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 28 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 29 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 30 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 31 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 32 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 33 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 34 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 35 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 36 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 37 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 38 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 39 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 40 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 41 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 42 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 43 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 44 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 45 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 46 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 47 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 48 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 49 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 50 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 51 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 52 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 53 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 54 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 55 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 56 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 57 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 58 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 59 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 60 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 61 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 62 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 63 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 64 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 65 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 66 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 67 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 68 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 69 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 70 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 71 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 72 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 73 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 74 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 75 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 76 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 77 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 78 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 79 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 80 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 81 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 82 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 83 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 84 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 85 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 86 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 87 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 88 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 89 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 90 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 91 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 92 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 93 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 94 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 95 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 96 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 97 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 98 | LOSS: 0.00000 | ACC: 1.0000\n",
      "EPOCHS: 99 | LOSS: 0.00000 | ACC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = NNBP(epochs=100, lr=0.1, n_hidden_layer_node=20)\n",
    "loss_lst3, acc_lst3 = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d908b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(loss_lst1, label='gd')\n",
    "plt.plot(loss_lst2, label='momentum')\n",
    "plt.plot(loss_lst3, label='AdaGrad')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
