{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5908d3",
   "metadata": {},
   "source": [
    "# 연습문제 5번\n",
    "\n",
    "## BP  ( Crossentropy Loss )\n",
    "\n",
    "## Neural Networks with a Single Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e24779",
   "metadata": {},
   "source": [
    "### load watermelon 3.0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32528a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('./data/chapter4_dataset.csv')\n",
    "df = pd.read_csv(data_dir)\n",
    "df = df.drop(['Idx'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d51f9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>root</th>\n",
       "      <th>knocks</th>\n",
       "      <th>texture</th>\n",
       "      <th>navel</th>\n",
       "      <th>touch</th>\n",
       "      <th>density</th>\n",
       "      <th>sugar_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dark_green</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dark_green</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>light_white</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dark_green</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>little_sinking</td>\n",
       "      <td>soft_stick</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>little_blur</td>\n",
       "      <td>little_sinking</td>\n",
       "      <td>soft_stick</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>black</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>little_sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>black</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>heavily</td>\n",
       "      <td>little_blur</td>\n",
       "      <td>little_sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dark_green</td>\n",
       "      <td>stiff</td>\n",
       "      <td>clear</td>\n",
       "      <td>distinct</td>\n",
       "      <td>even</td>\n",
       "      <td>soft_stick</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>light_white</td>\n",
       "      <td>stiff</td>\n",
       "      <td>clear</td>\n",
       "      <td>blur</td>\n",
       "      <td>even</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>light_white</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>blur</td>\n",
       "      <td>even</td>\n",
       "      <td>soft_stick</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dark_green</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>little_blur</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>light_white</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>heavily</td>\n",
       "      <td>little_blur</td>\n",
       "      <td>sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>black</td>\n",
       "      <td>little_curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>distinct</td>\n",
       "      <td>little_sinking</td>\n",
       "      <td>soft_stick</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>light_white</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>little_heavily</td>\n",
       "      <td>blur</td>\n",
       "      <td>even</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dark_green</td>\n",
       "      <td>curl_up</td>\n",
       "      <td>heavily</td>\n",
       "      <td>little_blur</td>\n",
       "      <td>little_sinking</td>\n",
       "      <td>hard_smooth</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          color            root          knocks      texture           navel  \\\n",
       "0    dark_green         curl_up  little_heavily     distinct         sinking   \n",
       "1         black         curl_up         heavily     distinct         sinking   \n",
       "2         black         curl_up  little_heavily     distinct         sinking   \n",
       "3    dark_green         curl_up         heavily     distinct         sinking   \n",
       "4   light_white         curl_up  little_heavily     distinct         sinking   \n",
       "5    dark_green  little_curl_up  little_heavily     distinct  little_sinking   \n",
       "6         black  little_curl_up  little_heavily  little_blur  little_sinking   \n",
       "7         black  little_curl_up  little_heavily     distinct  little_sinking   \n",
       "8         black  little_curl_up         heavily  little_blur  little_sinking   \n",
       "9    dark_green           stiff           clear     distinct            even   \n",
       "10  light_white           stiff           clear         blur            even   \n",
       "11  light_white         curl_up  little_heavily         blur            even   \n",
       "12   dark_green  little_curl_up  little_heavily  little_blur         sinking   \n",
       "13  light_white  little_curl_up         heavily  little_blur         sinking   \n",
       "14        black  little_curl_up  little_heavily     distinct  little_sinking   \n",
       "15  light_white         curl_up  little_heavily         blur            even   \n",
       "16   dark_green         curl_up         heavily  little_blur  little_sinking   \n",
       "\n",
       "          touch  density  sugar_ratio  label  \n",
       "0   hard_smooth    0.697        0.460      1  \n",
       "1   hard_smooth    0.774        0.376      1  \n",
       "2   hard_smooth    0.634        0.264      1  \n",
       "3   hard_smooth    0.608        0.318      1  \n",
       "4   hard_smooth    0.556        0.215      1  \n",
       "5    soft_stick    0.403        0.237      1  \n",
       "6    soft_stick    0.481        0.149      1  \n",
       "7   hard_smooth    0.437        0.211      1  \n",
       "8   hard_smooth    0.666        0.091      0  \n",
       "9    soft_stick    0.243        0.267      0  \n",
       "10  hard_smooth    0.245        0.057      0  \n",
       "11   soft_stick    0.343        0.099      0  \n",
       "12  hard_smooth    0.639        0.161      0  \n",
       "13  hard_smooth    0.657        0.198      0  \n",
       "14   soft_stick    0.360        0.370      0  \n",
       "15  hard_smooth    0.593        0.042      0  \n",
       "16  hard_smooth    0.719        0.103      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d281ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_att = ['color', 'root', 'knocks', 'texture', 'navel', 'touch']\n",
    "df = pd.get_dummies(df, columns=categorical_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e3cf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>density</th>\n",
       "      <th>sugar_ratio</th>\n",
       "      <th>label</th>\n",
       "      <th>color_black</th>\n",
       "      <th>color_dark_green</th>\n",
       "      <th>color_light_white</th>\n",
       "      <th>root_curl_up</th>\n",
       "      <th>root_little_curl_up</th>\n",
       "      <th>root_stiff</th>\n",
       "      <th>knocks_clear</th>\n",
       "      <th>knocks_heavily</th>\n",
       "      <th>knocks_little_heavily</th>\n",
       "      <th>texture_blur</th>\n",
       "      <th>texture_distinct</th>\n",
       "      <th>texture_little_blur</th>\n",
       "      <th>navel_even</th>\n",
       "      <th>navel_little_sinking</th>\n",
       "      <th>navel_sinking</th>\n",
       "      <th>touch_hard_smooth</th>\n",
       "      <th>touch_soft_stick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.697</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.634</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.608</td>\n",
       "      <td>0.318</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.556</td>\n",
       "      <td>0.215</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403</td>\n",
       "      <td>0.237</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.149</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.437</td>\n",
       "      <td>0.211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.666</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.243</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.245</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.343</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.657</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.593</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.719</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    density  sugar_ratio  label  color_black  color_dark_green  \\\n",
       "0     0.697        0.460      1            0                 1   \n",
       "1     0.774        0.376      1            1                 0   \n",
       "2     0.634        0.264      1            1                 0   \n",
       "3     0.608        0.318      1            0                 1   \n",
       "4     0.556        0.215      1            0                 0   \n",
       "5     0.403        0.237      1            0                 1   \n",
       "6     0.481        0.149      1            1                 0   \n",
       "7     0.437        0.211      1            1                 0   \n",
       "8     0.666        0.091      0            1                 0   \n",
       "9     0.243        0.267      0            0                 1   \n",
       "10    0.245        0.057      0            0                 0   \n",
       "11    0.343        0.099      0            0                 0   \n",
       "12    0.639        0.161      0            0                 1   \n",
       "13    0.657        0.198      0            0                 0   \n",
       "14    0.360        0.370      0            1                 0   \n",
       "15    0.593        0.042      0            0                 0   \n",
       "16    0.719        0.103      0            0                 1   \n",
       "\n",
       "    color_light_white  root_curl_up  root_little_curl_up  root_stiff  \\\n",
       "0                   0             1                    0           0   \n",
       "1                   0             1                    0           0   \n",
       "2                   0             1                    0           0   \n",
       "3                   0             1                    0           0   \n",
       "4                   1             1                    0           0   \n",
       "5                   0             0                    1           0   \n",
       "6                   0             0                    1           0   \n",
       "7                   0             0                    1           0   \n",
       "8                   0             0                    1           0   \n",
       "9                   0             0                    0           1   \n",
       "10                  1             0                    0           1   \n",
       "11                  1             1                    0           0   \n",
       "12                  0             0                    1           0   \n",
       "13                  1             0                    1           0   \n",
       "14                  0             0                    1           0   \n",
       "15                  1             1                    0           0   \n",
       "16                  0             1                    0           0   \n",
       "\n",
       "    knocks_clear  knocks_heavily  knocks_little_heavily  texture_blur  \\\n",
       "0              0               0                      1             0   \n",
       "1              0               1                      0             0   \n",
       "2              0               0                      1             0   \n",
       "3              0               1                      0             0   \n",
       "4              0               0                      1             0   \n",
       "5              0               0                      1             0   \n",
       "6              0               0                      1             0   \n",
       "7              0               0                      1             0   \n",
       "8              0               1                      0             0   \n",
       "9              1               0                      0             0   \n",
       "10             1               0                      0             1   \n",
       "11             0               0                      1             1   \n",
       "12             0               0                      1             0   \n",
       "13             0               1                      0             0   \n",
       "14             0               0                      1             0   \n",
       "15             0               0                      1             1   \n",
       "16             0               1                      0             0   \n",
       "\n",
       "    texture_distinct  texture_little_blur  navel_even  navel_little_sinking  \\\n",
       "0                  1                    0           0                     0   \n",
       "1                  1                    0           0                     0   \n",
       "2                  1                    0           0                     0   \n",
       "3                  1                    0           0                     0   \n",
       "4                  1                    0           0                     0   \n",
       "5                  1                    0           0                     1   \n",
       "6                  0                    1           0                     1   \n",
       "7                  1                    0           0                     1   \n",
       "8                  0                    1           0                     1   \n",
       "9                  1                    0           1                     0   \n",
       "10                 0                    0           1                     0   \n",
       "11                 0                    0           1                     0   \n",
       "12                 0                    1           0                     0   \n",
       "13                 0                    1           0                     0   \n",
       "14                 1                    0           0                     1   \n",
       "15                 0                    0           1                     0   \n",
       "16                 0                    1           0                     1   \n",
       "\n",
       "    navel_sinking  touch_hard_smooth  touch_soft_stick  \n",
       "0               1                  1                 0  \n",
       "1               1                  1                 0  \n",
       "2               1                  1                 0  \n",
       "3               1                  1                 0  \n",
       "4               1                  1                 0  \n",
       "5               0                  0                 1  \n",
       "6               0                  0                 1  \n",
       "7               0                  1                 0  \n",
       "8               0                  1                 0  \n",
       "9               0                  0                 1  \n",
       "10              0                  1                 0  \n",
       "11              0                  0                 1  \n",
       "12              1                  1                 0  \n",
       "13              1                  1                 0  \n",
       "14              0                  0                 1  \n",
       "15              0                  1                 0  \n",
       "16              0                  1                 0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba992183",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.drop(['label'], axis=1).values, df['label'].values[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19537c23",
   "metadata": {},
   "source": [
    "### 표준 BP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f3c63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBP:\n",
    "    def __init__(self, lr=0.1, epochs=100, n_hidden_layer_node=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_hidden_layer_node = n_hidden_layer_node\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def update(self, x_, y_):\n",
    "        x2 = self.sigmoid(np.dot(x_, self.w1) + self.b1) # 1 x n_hidden_layer_node\n",
    "        y2 = self.sigmoid(np.dot(x2, self.w2) + self.b2) # 1 x n_output_node\n",
    "        \n",
    "        y2y_diff = y2 - y_\n",
    "        \n",
    "        grad_b1 = np.dot(y2y_diff, self.w2.T) * x2 * (1 - x2)\n",
    "        grad_w1 = np.dot(x_.T, grad_b1)\n",
    "        grad_b2 = y2y_diff\n",
    "        grad_w2 = np.dot(x2.T , y2y_diff)\n",
    "        \n",
    "        self.b1 -= self.lr * grad_b1\n",
    "        self.w1 -= self.lr * grad_w1\n",
    "        self.b2 -= self.lr * grad_b2\n",
    "        self.w2 -= self.lr * grad_w2\n",
    "        \n",
    "    def crossentropy(self, y_hat, y_):\n",
    "        return np.mean(-y_ * np.log(y_hat) - (1 - y_) * np.log(1 - y_hat))\n",
    "    \n",
    "    def accuracy(self, y_hat, y_):\n",
    "        return np.mean((y_hat >= 0.5).astype(np.int) == y_)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        n_input_node = x.shape[-1]\n",
    "        n_output_node = y.shape[-1]\n",
    "        \n",
    "        self.length = len(x)\n",
    "        \n",
    "        self.w1 = np.random.randn(n_input_node, self.n_hidden_layer_node)\n",
    "        self.b1 = np.zeros((1, self.n_hidden_layer_node))\n",
    "        \n",
    "        self.w2 = np.random.randn(self.n_hidden_layer_node, n_output_node)\n",
    "        self.b2 = np.zeros((1, n_output_node))\n",
    "        \n",
    "        loss_lst = []\n",
    "        acc_lst = []\n",
    "        \n",
    "        for j in range(self.epochs):\n",
    "            for i in range(self.length):\n",
    "                x_, y_ = x[i:i+1], y[i:i+1]  # 1x19  1x1\n",
    "\n",
    "                self.update(x_, y_)\n",
    "\n",
    "            pred = self.predict(x)\n",
    "            loss = self.crossentropy(pred, y)\n",
    "            acc = self.accuracy(pred, y)\n",
    "            \n",
    "            loss_lst += [loss]\n",
    "            acc_lst += [acc]\n",
    "            \n",
    "            print(\"EPOCHS: {} | LOSS: {} | ACC: {}\".format(j, loss, acc))\n",
    "            \n",
    "        return loss_lst, acc_lst\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.sigmoid(np.dot(self.sigmoid(np.dot(x, self.w1) + self.b1), self.w2) + self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ffe9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinjy\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 0 | LOSS: 0.7742924231747406 | ACC: 0.5294117647058824\n",
      "EPOCHS: 1 | LOSS: 0.7476234382456657 | ACC: 0.5294117647058824\n",
      "EPOCHS: 2 | LOSS: 0.7153695688535481 | ACC: 0.5882352941176471\n",
      "EPOCHS: 3 | LOSS: 0.6834357452950413 | ACC: 0.7058823529411765\n",
      "EPOCHS: 4 | LOSS: 0.6527930462751412 | ACC: 0.7058823529411765\n",
      "EPOCHS: 5 | LOSS: 0.6235496897056363 | ACC: 0.7058823529411765\n",
      "EPOCHS: 6 | LOSS: 0.5956980290333542 | ACC: 0.7058823529411765\n",
      "EPOCHS: 7 | LOSS: 0.5692094589728668 | ACC: 0.7058823529411765\n",
      "EPOCHS: 8 | LOSS: 0.5440393327925355 | ACC: 0.7058823529411765\n",
      "EPOCHS: 9 | LOSS: 0.5201336908984755 | ACC: 0.7058823529411765\n",
      "EPOCHS: 10 | LOSS: 0.49744159920015923 | ACC: 0.7058823529411765\n",
      "EPOCHS: 11 | LOSS: 0.4759260061464738 | ACC: 0.7058823529411765\n",
      "EPOCHS: 12 | LOSS: 0.45556823150138626 | ACC: 0.7647058823529411\n",
      "EPOCHS: 13 | LOSS: 0.4363644222081514 | ACC: 0.8823529411764706\n",
      "EPOCHS: 14 | LOSS: 0.41831541931566757 | ACC: 0.8823529411764706\n",
      "EPOCHS: 15 | LOSS: 0.40141419230694336 | ACC: 0.8823529411764706\n",
      "EPOCHS: 16 | LOSS: 0.38563600659221553 | ACC: 0.8823529411764706\n",
      "EPOCHS: 17 | LOSS: 0.3709348684389051 | ACC: 0.8823529411764706\n",
      "EPOCHS: 18 | LOSS: 0.3572465087882065 | ACC: 0.8823529411764706\n",
      "EPOCHS: 19 | LOSS: 0.34449542893717056 | ACC: 0.8823529411764706\n",
      "EPOCHS: 20 | LOSS: 0.33260274108234417 | ACC: 0.8823529411764706\n",
      "EPOCHS: 21 | LOSS: 0.32149241251293814 | ACC: 0.8823529411764706\n",
      "EPOCHS: 22 | LOSS: 0.31109494738212323 | ACC: 0.8823529411764706\n",
      "EPOCHS: 23 | LOSS: 0.30134866776760455 | ACC: 0.9411764705882353\n",
      "EPOCHS: 24 | LOSS: 0.29219932728590925 | ACC: 0.9411764705882353\n",
      "EPOCHS: 25 | LOSS: 0.2835988992402498 | ACC: 0.9411764705882353\n",
      "EPOCHS: 26 | LOSS: 0.2755042128098992 | ACC: 0.9411764705882353\n",
      "EPOCHS: 27 | LOSS: 0.2678758338000323 | ACC: 0.9411764705882353\n",
      "EPOCHS: 28 | LOSS: 0.2606773257390103 | ACC: 0.9411764705882353\n",
      "EPOCHS: 29 | LOSS: 0.2538748521569399 | ACC: 0.9411764705882353\n",
      "EPOCHS: 30 | LOSS: 0.24743700492848397 | ACC: 0.9411764705882353\n",
      "EPOCHS: 31 | LOSS: 0.24133474099078023 | ACC: 0.9411764705882353\n",
      "EPOCHS: 32 | LOSS: 0.23554134290312312 | ACC: 0.9411764705882353\n",
      "EPOCHS: 33 | LOSS: 0.2300323579617311 | ACC: 0.9411764705882353\n",
      "EPOCHS: 34 | LOSS: 0.22478550085304247 | ACC: 0.9411764705882353\n",
      "EPOCHS: 35 | LOSS: 0.219780522636759 | ACC: 0.9411764705882353\n",
      "EPOCHS: 36 | LOSS: 0.21499905655200893 | ACC: 0.9411764705882353\n",
      "EPOCHS: 37 | LOSS: 0.2104244525640219 | ACC: 0.9411764705882353\n",
      "EPOCHS: 38 | LOSS: 0.20604161090539522 | ACC: 0.9411764705882353\n",
      "EPOCHS: 39 | LOSS: 0.20183682214781415 | ACC: 0.9411764705882353\n",
      "EPOCHS: 40 | LOSS: 0.19779761863038337 | ACC: 0.9411764705882353\n",
      "EPOCHS: 41 | LOSS: 0.1939126398147146 | ACC: 0.9411764705882353\n",
      "EPOCHS: 42 | LOSS: 0.1901715124485579 | ACC: 0.9411764705882353\n",
      "EPOCHS: 43 | LOSS: 0.18656474526159386 | ACC: 0.9411764705882353\n",
      "EPOCHS: 44 | LOSS: 0.1830836371959917 | ACC: 0.9411764705882353\n",
      "EPOCHS: 45 | LOSS: 0.1797201977871354 | ACC: 0.9411764705882353\n",
      "EPOCHS: 46 | LOSS: 0.17646707816192375 | ACC: 1.0\n",
      "EPOCHS: 47 | LOSS: 0.1733175111341068 | ACC: 1.0\n",
      "EPOCHS: 48 | LOSS: 0.1702652589860072 | ACC: 1.0\n",
      "EPOCHS: 49 | LOSS: 0.1673045676872952 | ACC: 1.0\n",
      "EPOCHS: 50 | LOSS: 0.16443012648168454 | ACC: 1.0\n",
      "EPOCHS: 51 | LOSS: 0.16163703195019585 | ACC: 1.0\n",
      "EPOCHS: 52 | LOSS: 0.1589207558224489 | ACC: 1.0\n",
      "EPOCHS: 53 | LOSS: 0.15627711594905133 | ACC: 1.0\n",
      "EPOCHS: 54 | LOSS: 0.1537022499666338 | ACC: 1.0\n",
      "EPOCHS: 55 | LOSS: 0.15119259128320195 | ACC: 1.0\n",
      "EPOCHS: 56 | LOSS: 0.14874484708747124 | ACC: 1.0\n",
      "EPOCHS: 57 | LOSS: 0.14635597814463117 | ACC: 1.0\n",
      "EPOCHS: 58 | LOSS: 0.14402318018554505 | ACC: 1.0\n",
      "EPOCHS: 59 | LOSS: 0.14174386672957176 | ACC: 1.0\n",
      "EPOCHS: 60 | LOSS: 0.13951565320546333 | ACC: 1.0\n",
      "EPOCHS: 61 | LOSS: 0.13733634225225935 | ACC: 1.0\n",
      "EPOCHS: 62 | LOSS: 0.1352039100944825 | ACC: 1.0\n",
      "EPOCHS: 63 | LOSS: 0.13311649389466068 | ACC: 1.0\n",
      "EPOCHS: 64 | LOSS: 0.1310723799923752 | ACC: 1.0\n",
      "EPOCHS: 65 | LOSS: 0.12906999294361382 | ACC: 1.0\n",
      "EPOCHS: 66 | LOSS: 0.1271078852779254 | ACC: 1.0\n",
      "EPOCHS: 67 | LOSS: 0.1251847278943783 | ACC: 1.0\n",
      "EPOCHS: 68 | LOSS: 0.12329930102111852 | ACC: 1.0\n",
      "EPOCHS: 69 | LOSS: 0.12145048566783057 | ACC: 1.0\n",
      "EPOCHS: 70 | LOSS: 0.1196372555059411 | ACC: 1.0\n",
      "EPOCHS: 71 | LOSS: 0.11785866911816945 | ACC: 1.0\n",
      "EPOCHS: 72 | LOSS: 0.11611386256712206 | ACC: 1.0\n",
      "EPOCHS: 73 | LOSS: 0.11440204224199595 | ACC: 1.0\n",
      "EPOCHS: 74 | LOSS: 0.11272247795292538 | ACC: 1.0\n",
      "EPOCHS: 75 | LOSS: 0.111074496253766 | ACC: 1.0\n",
      "EPOCHS: 76 | LOSS: 0.10945747398571654 | ACC: 1.0\n",
      "EPOCHS: 77 | LOSS: 0.10787083204560863 | ACC: 1.0\n",
      "EPOCHS: 78 | LOSS: 0.10631402939333995 | ACC: 1.0\n",
      "EPOCHS: 79 | LOSS: 0.10478655732219322 | ACC: 1.0\n",
      "EPOCHS: 80 | LOSS: 0.10328793402309445 | ACC: 1.0\n",
      "EPOCHS: 81 | LOSS: 0.10181769947875076 | ACC: 1.0\n",
      "EPOCHS: 82 | LOSS: 0.1003754107257418 | ACC: 1.0\n",
      "EPOCHS: 83 | LOSS: 0.09896063752187632 | ACC: 1.0\n",
      "EPOCHS: 84 | LOSS: 0.09757295845251641 | ACC: 1.0\n",
      "EPOCHS: 85 | LOSS: 0.09621195750337966 | ACC: 1.0\n",
      "EPOCHS: 86 | LOSS: 0.09487722111899334 | ACC: 1.0\n",
      "EPOCHS: 87 | LOSS: 0.09356833575607493 | ACC: 1.0\n",
      "EPOCHS: 88 | LOSS: 0.09228488593032823 | ACC: 1.0\n",
      "EPOCHS: 89 | LOSS: 0.09102645274417784 | ACC: 1.0\n",
      "EPOCHS: 90 | LOSS: 0.0897926128724903 | ACC: 1.0\n",
      "EPOCHS: 91 | LOSS: 0.088582937973953 | ACC: 1.0\n",
      "EPOCHS: 92 | LOSS: 0.08739699448795762 | ACC: 1.0\n",
      "EPOCHS: 93 | LOSS: 0.08623434377090981 | ACC: 1.0\n",
      "EPOCHS: 94 | LOSS: 0.08509454252200173 | ACC: 1.0\n",
      "EPOCHS: 95 | LOSS: 0.08397714344667934 | ACC: 1.0\n",
      "EPOCHS: 96 | LOSS: 0.08288169610617281 | ACC: 1.0\n",
      "EPOCHS: 97 | LOSS: 0.0818077479033289 | ACC: 1.0\n",
      "EPOCHS: 98 | LOSS: 0.08075484515828243 | ACC: 1.0\n",
      "EPOCHS: 99 | LOSS: 0.07972253423189858 | ACC: 1.0\n",
      "EPOCHS: 100 | LOSS: 0.07871036266006536 | ACC: 1.0\n",
      "EPOCHS: 101 | LOSS: 0.07771788026747482 | ACC: 1.0\n",
      "EPOCHS: 102 | LOSS: 0.07674464023521903 | ACC: 1.0\n",
      "EPOCHS: 103 | LOSS: 0.07579020010208155 | ACC: 1.0\n",
      "EPOCHS: 104 | LOSS: 0.07485412268463357 | ACC: 1.0\n",
      "EPOCHS: 105 | LOSS: 0.07393597690600578 | ACC: 1.0\n",
      "EPOCHS: 106 | LOSS: 0.07303533852740077 | ACC: 1.0\n",
      "EPOCHS: 107 | LOSS: 0.0721517907800019 | ACC: 1.0\n",
      "EPOCHS: 108 | LOSS: 0.07128492489790067 | ACC: 1.0\n",
      "EPOCHS: 109 | LOSS: 0.07043434055503284 | ACC: 1.0\n",
      "EPOCHS: 110 | LOSS: 0.06959964621092533 | ACC: 1.0\n",
      "EPOCHS: 111 | LOSS: 0.06878045937136341 | ACC: 1.0\n",
      "EPOCHS: 112 | LOSS: 0.06797640677095643 | ACC: 1.0\n",
      "EPOCHS: 113 | LOSS: 0.06718712448507962 | ACC: 1.0\n",
      "EPOCHS: 114 | LOSS: 0.06641225797886063 | ACC: 1.0\n",
      "EPOCHS: 115 | LOSS: 0.06565146210082784 | ACC: 1.0\n",
      "EPOCHS: 116 | LOSS: 0.06490440102859804 | ACC: 1.0\n",
      "EPOCHS: 117 | LOSS: 0.0641707481736051 | ACC: 1.0\n",
      "EPOCHS: 118 | LOSS: 0.06345018605139892 | ACC: 1.0\n",
      "EPOCHS: 119 | LOSS: 0.06274240612351244 | ACC: 1.0\n",
      "EPOCHS: 120 | LOSS: 0.062047108616333016 | ACC: 1.0\n",
      "EPOCHS: 121 | LOSS: 0.06136400232184654 | ACC: 1.0\n",
      "EPOCHS: 122 | LOSS: 0.06069280438455937 | ACC: 1.0\n",
      "EPOCHS: 123 | LOSS: 0.06003324007837013 | ACC: 1.0\n",
      "EPOCHS: 124 | LOSS: 0.05938504257665492 | ACC: 1.0\n",
      "EPOCHS: 125 | LOSS: 0.058747952718362334 | ACC: 1.0\n",
      "EPOCHS: 126 | LOSS: 0.058121718772485904 | ACC: 1.0\n",
      "EPOCHS: 127 | LOSS: 0.057506096202895915 | ACC: 1.0\n",
      "EPOCHS: 128 | LOSS: 0.05690084743516706 | ACC: 1.0\n",
      "EPOCHS: 129 | LOSS: 0.05630574162673343 | ACC: 1.0\n",
      "EPOCHS: 130 | LOSS: 0.05572055444143562 | ACC: 1.0\n",
      "EPOCHS: 131 | LOSS: 0.055145067829292134 | ACC: 1.0\n",
      "EPOCHS: 132 | LOSS: 0.05457906981212875 | ACC: 1.0\n",
      "EPOCHS: 133 | LOSS: 0.054022354275528026 | ACC: 1.0\n",
      "EPOCHS: 134 | LOSS: 0.053474720767419236 | ACC: 1.0\n",
      "EPOCHS: 135 | LOSS: 0.052935974303505696 | ACC: 1.0\n",
      "EPOCHS: 136 | LOSS: 0.05240592517962944 | ACC: 1.0\n",
      "EPOCHS: 137 | LOSS: 0.051884388791089336 | ACC: 1.0\n",
      "EPOCHS: 138 | LOSS: 0.0513711854588631 | ACC: 1.0\n",
      "EPOCHS: 139 | LOSS: 0.05086614026263015 | ACC: 1.0\n",
      "EPOCHS: 140 | LOSS: 0.05036908288045272 | ACC: 1.0\n",
      "EPOCHS: 141 | LOSS: 0.049879847434936744 | ACC: 1.0\n",
      "EPOCHS: 142 | LOSS: 0.049398272345676245 | ACC: 1.0\n",
      "EPOCHS: 143 | LOSS: 0.04892420018776249 | ACC: 1.0\n",
      "EPOCHS: 144 | LOSS: 0.04845747755613243 | ACC: 1.0\n",
      "EPOCHS: 145 | LOSS: 0.04799795493552281 | ACC: 1.0\n",
      "EPOCHS: 146 | LOSS: 0.047545486575792634 | ACC: 1.0\n",
      "EPOCHS: 147 | LOSS: 0.04709993037238058 | ACC: 1.0\n",
      "EPOCHS: 148 | LOSS: 0.04666114775166236 | ACC: 1.0\n",
      "EPOCHS: 149 | LOSS: 0.04622900356098228 | ACC: 1.0\n",
      "EPOCHS: 150 | LOSS: 0.04580336596313568 | ACC: 1.0\n",
      "EPOCHS: 151 | LOSS: 0.04538410633508871 | ACC: 1.0\n",
      "EPOCHS: 152 | LOSS: 0.04497109917072892 | ACC: 1.0\n",
      "EPOCHS: 153 | LOSS: 0.044564221987448444 | ACC: 1.0\n",
      "EPOCHS: 154 | LOSS: 0.044163355236371375 | ACC: 1.0\n",
      "EPOCHS: 155 | LOSS: 0.04376838221604409 | ACC: 1.0\n",
      "EPOCHS: 156 | LOSS: 0.043379188989418394 | ACC: 1.0\n",
      "EPOCHS: 157 | LOSS: 0.042995664303964704 | ACC: 1.0\n",
      "EPOCHS: 158 | LOSS: 0.0426176995147606 | ACC: 1.0\n",
      "EPOCHS: 159 | LOSS: 0.04224518851041091 | ACC: 1.0\n",
      "EPOCHS: 160 | LOSS: 0.041878027641660515 | ACC: 1.0\n",
      "EPOCHS: 161 | LOSS: 0.041516115652570705 | ACC: 1.0\n",
      "EPOCHS: 162 | LOSS: 0.04115935361413706 | ACC: 1.0\n",
      "EPOCHS: 163 | LOSS: 0.04080764486023276 | ACC: 1.0\n",
      "EPOCHS: 164 | LOSS: 0.040460894925770007 | ACC: 1.0\n",
      "EPOCHS: 165 | LOSS: 0.040119011486975886 | ACC: 1.0\n",
      "EPOCHS: 166 | LOSS: 0.03978190430368701 | ACC: 1.0\n",
      "EPOCHS: 167 | LOSS: 0.039449485163572086 | ACC: 1.0\n",
      "EPOCHS: 168 | LOSS: 0.039121667828196316 | ACC: 1.0\n",
      "EPOCHS: 169 | LOSS: 0.03879836798084799 | ACC: 1.0\n",
      "EPOCHS: 170 | LOSS: 0.038479503176050124 | ACC: 1.0\n",
      "EPOCHS: 171 | LOSS: 0.03816499279068602 | ACC: 1.0\n",
      "EPOCHS: 172 | LOSS: 0.03785475797667139 | ACC: 1.0\n",
      "EPOCHS: 173 | LOSS: 0.03754872161510835 | ACC: 1.0\n",
      "EPOCHS: 174 | LOSS: 0.03724680827186132 | ACC: 1.0\n",
      "EPOCHS: 175 | LOSS: 0.03694894415449817 | ACC: 1.0\n",
      "EPOCHS: 176 | LOSS: 0.03665505707054181 | ACC: 1.0\n",
      "EPOCHS: 177 | LOSS: 0.0363650763869823 | ACC: 1.0\n",
      "EPOCHS: 178 | LOSS: 0.03607893299099928 | ACC: 1.0\n",
      "EPOCHS: 179 | LOSS: 0.035796559251850886 | ACC: 1.0\n",
      "EPOCHS: 180 | LOSS: 0.035517888983884055 | ACC: 1.0\n",
      "EPOCHS: 181 | LOSS: 0.035242857410625786 | ACC: 1.0\n",
      "EPOCHS: 182 | LOSS: 0.03497140112991529 | ACC: 1.0\n",
      "EPOCHS: 183 | LOSS: 0.03470345808004036 | ACC: 1.0\n",
      "EPOCHS: 184 | LOSS: 0.034438967506842125 | ACC: 1.0\n",
      "EPOCHS: 185 | LOSS: 0.03417786993175391 | ACC: 1.0\n",
      "EPOCHS: 186 | LOSS: 0.033920107120742504 | ACC: 1.0\n",
      "EPOCHS: 187 | LOSS: 0.03366562205412061 | ACC: 1.0\n",
      "EPOCHS: 188 | LOSS: 0.0334143588972012 | ACC: 1.0\n",
      "EPOCHS: 189 | LOSS: 0.03316626297176534 | ACC: 1.0\n",
      "EPOCHS: 190 | LOSS: 0.03292128072831691 | ACC: 1.0\n",
      "EPOCHS: 191 | LOSS: 0.032679359719098466 | ACC: 1.0\n",
      "EPOCHS: 192 | LOSS: 0.03244044857184321 | ACC: 1.0\n",
      "EPOCHS: 193 | LOSS: 0.032204496964239805 | ACC: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 194 | LOSS: 0.031971455599087116 | ACC: 1.0\n",
      "EPOCHS: 195 | LOSS: 0.03174127618011719 | ACC: 1.0\n",
      "EPOCHS: 196 | LOSS: 0.03151391138846571 | ACC: 1.0\n",
      "EPOCHS: 197 | LOSS: 0.031289314859769705 | ACC: 1.0\n",
      "EPOCHS: 198 | LOSS: 0.031067441161873086 | ACC: 1.0\n",
      "EPOCHS: 199 | LOSS: 0.030848245773121626 | ACC: 1.0\n",
      "EPOCHS: 200 | LOSS: 0.03063168506122981 | ACC: 1.0\n",
      "EPOCHS: 201 | LOSS: 0.030417716262701483 | ACC: 1.0\n",
      "EPOCHS: 202 | LOSS: 0.03020629746278865 | ACC: 1.0\n",
      "EPOCHS: 203 | LOSS: 0.029997387575971843 | ACC: 1.0\n",
      "EPOCHS: 204 | LOSS: 0.029790946326946937 | ACC: 1.0\n",
      "EPOCHS: 205 | LOSS: 0.029586934232103484 | ACC: 1.0\n",
      "EPOCHS: 206 | LOSS: 0.02938531258148013 | ACC: 1.0\n",
      "EPOCHS: 207 | LOSS: 0.029186043421183364 | ACC: 1.0\n",
      "EPOCHS: 208 | LOSS: 0.028989089536256035 | ACC: 1.0\n",
      "EPOCHS: 209 | LOSS: 0.02879441443398335 | ACC: 1.0\n",
      "EPOCHS: 210 | LOSS: 0.028601982327622722 | ACC: 1.0\n",
      "EPOCHS: 211 | LOSS: 0.028411758120546454 | ACC: 1.0\n",
      "EPOCHS: 212 | LOSS: 0.028223707390785346 | ACC: 1.0\n",
      "EPOCHS: 213 | LOSS: 0.02803779637596101 | ACC: 1.0\n",
      "EPOCHS: 214 | LOSS: 0.02785399195859751 | ACC: 1.0\n",
      "EPOCHS: 215 | LOSS: 0.02767226165180047 | ACC: 1.0\n",
      "EPOCHS: 216 | LOSS: 0.027492573585294076 | ACC: 1.0\n",
      "EPOCHS: 217 | LOSS: 0.027314896491805657 | ACC: 1.0\n",
      "EPOCHS: 218 | LOSS: 0.02713919969378846 | ACC: 1.0\n",
      "EPOCHS: 219 | LOSS: 0.026965453090473487 | ACC: 1.0\n",
      "EPOCHS: 220 | LOSS: 0.026793627145240684 | ACC: 1.0\n",
      "EPOCHS: 221 | LOSS: 0.026623692873301828 | ACC: 1.0\n",
      "EPOCHS: 222 | LOSS: 0.026455621829685295 | ACC: 1.0\n",
      "EPOCHS: 223 | LOSS: 0.02628938609751578 | ACC: 1.0\n",
      "EPOCHS: 224 | LOSS: 0.02612495827657992 | ACC: 1.0\n",
      "EPOCHS: 225 | LOSS: 0.02596231147217123 | ACC: 1.0\n",
      "EPOCHS: 226 | LOSS: 0.02580141928420549 | ACC: 1.0\n",
      "EPOCHS: 227 | LOSS: 0.025642255796600455 | ACC: 1.0\n",
      "EPOCHS: 228 | LOSS: 0.02548479556691219 | ACC: 1.0\n",
      "EPOCHS: 229 | LOSS: 0.02532901361622162 | ACC: 1.0\n",
      "EPOCHS: 230 | LOSS: 0.02517488541926413 | ACC: 1.0\n",
      "EPOCHS: 231 | LOSS: 0.02502238689479629 | ACC: 1.0\n",
      "EPOCHS: 232 | LOSS: 0.02487149439619277 | ACC: 1.0\n",
      "EPOCHS: 233 | LOSS: 0.02472218470226837 | ACC: 1.0\n",
      "EPOCHS: 234 | LOSS: 0.024574435008318032 | ACC: 1.0\n",
      "EPOCHS: 235 | LOSS: 0.024428222917370332 | ACC: 1.0\n",
      "EPOCHS: 236 | LOSS: 0.02428352643164757 | ACC: 1.0\n",
      "EPOCHS: 237 | LOSS: 0.02414032394422839 | ACC: 1.0\n",
      "EPOCHS: 238 | LOSS: 0.023998594230906646 | ACC: 1.0\n",
      "EPOCHS: 239 | LOSS: 0.023858316442241814 | ACC: 1.0\n",
      "EPOCHS: 240 | LOSS: 0.023719470095795985 | ACC: 1.0\n",
      "EPOCHS: 241 | LOSS: 0.023582035068552658 | ACC: 1.0\n",
      "EPOCHS: 242 | LOSS: 0.02344599158951238 | ACC: 1.0\n",
      "EPOCHS: 243 | LOSS: 0.023311320232461047 | ACC: 1.0\n",
      "EPOCHS: 244 | LOSS: 0.023178001908906003 | ACC: 1.0\n",
      "EPOCHS: 245 | LOSS: 0.023046017861176122 | ACC: 1.0\n",
      "EPOCHS: 246 | LOSS: 0.02291534965568128 | ACC: 1.0\n",
      "EPOCHS: 247 | LOSS: 0.022785979176327287 | ACC: 1.0\n",
      "EPOCHS: 248 | LOSS: 0.022657888618082486 | ACC: 1.0\n",
      "EPOCHS: 249 | LOSS: 0.022531060480691867 | ACC: 1.0\n",
      "EPOCHS: 250 | LOSS: 0.022405477562535133 | ACC: 1.0\n",
      "EPOCHS: 251 | LOSS: 0.022281122954625334 | ACC: 1.0\n",
      "EPOCHS: 252 | LOSS: 0.022157980034743566 | ACC: 1.0\n",
      "EPOCHS: 253 | LOSS: 0.0220360324617079 | ACC: 1.0\n",
      "EPOCHS: 254 | LOSS: 0.02191526416977152 | ACC: 1.0\n",
      "EPOCHS: 255 | LOSS: 0.02179565936314798 | ACC: 1.0\n",
      "EPOCHS: 256 | LOSS: 0.02167720251065982 | ACC: 1.0\n",
      "EPOCHS: 257 | LOSS: 0.021559878340507686 | ACC: 1.0\n",
      "EPOCHS: 258 | LOSS: 0.021443671835157142 | ACC: 1.0\n",
      "EPOCHS: 259 | LOSS: 0.021328568226339415 | ACC: 1.0\n",
      "EPOCHS: 260 | LOSS: 0.02121455299016462 | ACC: 1.0\n",
      "EPOCHS: 261 | LOSS: 0.021101611842343337 | ACC: 1.0\n",
      "EPOCHS: 262 | LOSS: 0.02098973073351474 | ACC: 1.0\n",
      "EPOCHS: 263 | LOSS: 0.020878895844678235 | ACC: 1.0\n",
      "EPOCHS: 264 | LOSS: 0.020769093582726307 | ACC: 1.0\n",
      "EPOCHS: 265 | LOSS: 0.020660310576075907 | ACC: 1.0\n",
      "EPOCHS: 266 | LOSS: 0.020552533670395973 | ACC: 1.0\n",
      "EPOCHS: 267 | LOSS: 0.02044574992442908 | ACC: 1.0\n",
      "EPOCHS: 268 | LOSS: 0.020339946605904262 | ACC: 1.0\n",
      "EPOCHS: 269 | LOSS: 0.02023511118753944 | ACC: 1.0\n",
      "EPOCHS: 270 | LOSS: 0.020131231343131252 | ACC: 1.0\n",
      "EPOCHS: 271 | LOSS: 0.020028294943729232 | ACC: 1.0\n",
      "EPOCHS: 272 | LOSS: 0.019926290053893826 | ACC: 1.0\n",
      "EPOCHS: 273 | LOSS: 0.01982520492803487 | ACC: 1.0\n",
      "EPOCHS: 274 | LOSS: 0.019725028006829466 | ACC: 1.0\n",
      "EPOCHS: 275 | LOSS: 0.019625747913716768 | ACC: 1.0\n",
      "EPOCHS: 276 | LOSS: 0.01952735345146825 | ACC: 1.0\n",
      "EPOCHS: 277 | LOSS: 0.019429833598831592 | ACC: 1.0\n",
      "EPOCHS: 278 | LOSS: 0.019333177507246006 | ACC: 1.0\n",
      "EPOCHS: 279 | LOSS: 0.01923737449762814 | ACC: 1.0\n",
      "EPOCHS: 280 | LOSS: 0.01914241405722575 | ACC: 1.0\n",
      "EPOCHS: 281 | LOSS: 0.0190482858365388 | ACC: 1.0\n",
      "EPOCHS: 282 | LOSS: 0.018954979646305395 | ACC: 1.0\n",
      "EPOCHS: 283 | LOSS: 0.01886248545455142 | ACC: 1.0\n",
      "EPOCHS: 284 | LOSS: 0.01877079338370244 | ACC: 1.0\n",
      "EPOCHS: 285 | LOSS: 0.018679893707756252 | ACC: 1.0\n",
      "EPOCHS: 286 | LOSS: 0.018589776849514933 | ACC: 1.0\n",
      "EPOCHS: 287 | LOSS: 0.018500433377874353 | ACC: 1.0\n",
      "EPOCHS: 288 | LOSS: 0.018411854005170716 | ACC: 1.0\n",
      "EPOCHS: 289 | LOSS: 0.018324029584582163 | ACC: 1.0\n",
      "EPOCHS: 290 | LOSS: 0.01823695110758415 | ACC: 1.0\n",
      "EPOCHS: 291 | LOSS: 0.018150609701457966 | ACC: 1.0\n",
      "EPOCHS: 292 | LOSS: 0.01806499662685021 | ACC: 1.0\n",
      "EPOCHS: 293 | LOSS: 0.017980103275383134 | ACC: 1.0\n",
      "EPOCHS: 294 | LOSS: 0.01789592116731358 | ACC: 1.0\n",
      "EPOCHS: 295 | LOSS: 0.01781244194924045 | ACC: 1.0\n",
      "EPOCHS: 296 | LOSS: 0.017729657391858698 | ACC: 1.0\n",
      "EPOCHS: 297 | LOSS: 0.01764755938775938 | ACC: 1.0\n",
      "EPOCHS: 298 | LOSS: 0.017566139949274508 | ACC: 1.0\n",
      "EPOCHS: 299 | LOSS: 0.017485391206365452 | ACC: 1.0\n",
      "EPOCHS: 300 | LOSS: 0.01740530540455455 | ACC: 1.0\n",
      "EPOCHS: 301 | LOSS: 0.0173258749028981 | ACC: 1.0\n",
      "EPOCHS: 302 | LOSS: 0.01724709217200052 | ACC: 1.0\n",
      "EPOCHS: 303 | LOSS: 0.017168949792068282 | ACC: 1.0\n",
      "EPOCHS: 304 | LOSS: 0.017091440451003068 | ACC: 1.0\n",
      "EPOCHS: 305 | LOSS: 0.017014556942532885 | ACC: 1.0\n",
      "EPOCHS: 306 | LOSS: 0.016938292164380725 | ACC: 1.0\n",
      "EPOCHS: 307 | LOSS: 0.016862639116469517 | ACC: 1.0\n",
      "EPOCHS: 308 | LOSS: 0.016787590899162885 | ACC: 1.0\n",
      "EPOCHS: 309 | LOSS: 0.01671314071154064 | ACC: 1.0\n",
      "EPOCHS: 310 | LOSS: 0.016639281849708632 | ACC: 1.0\n",
      "EPOCHS: 311 | LOSS: 0.016566007705141734 | ACC: 1.0\n",
      "EPOCHS: 312 | LOSS: 0.01649331176305921 | ACC: 1.0\n",
      "EPOCHS: 313 | LOSS: 0.016421187600832526 | ACC: 1.0\n",
      "EPOCHS: 314 | LOSS: 0.016349628886424095 | ACC: 1.0\n",
      "EPOCHS: 315 | LOSS: 0.01627862937685632 | ACC: 1.0\n",
      "EPOCHS: 316 | LOSS: 0.01620818291671065 | ACC: 1.0\n",
      "EPOCHS: 317 | LOSS: 0.01613828343665589 | ACC: 1.0\n",
      "EPOCHS: 318 | LOSS: 0.016068924952004855 | ACC: 1.0\n",
      "EPOCHS: 319 | LOSS: 0.016000101561299336 | ACC: 1.0\n",
      "EPOCHS: 320 | LOSS: 0.015931807444921675 | ACC: 1.0\n",
      "EPOCHS: 321 | LOSS: 0.015864036863733836 | ACC: 1.0\n",
      "EPOCHS: 322 | LOSS: 0.01579678415774213 | ACC: 1.0\n",
      "EPOCHS: 323 | LOSS: 0.015730043744787713 | ACC: 1.0\n",
      "EPOCHS: 324 | LOSS: 0.01566381011926207 | ACC: 1.0\n",
      "EPOCHS: 325 | LOSS: 0.015598077850846947 | ACC: 1.0\n",
      "EPOCHS: 326 | LOSS: 0.015532841583278367 | ACC: 1.0\n",
      "EPOCHS: 327 | LOSS: 0.015468096033134045 | ACC: 1.0\n",
      "EPOCHS: 328 | LOSS: 0.015403835988643488 | ACC: 1.0\n",
      "EPOCHS: 329 | LOSS: 0.01534005630852116 | ACC: 1.0\n",
      "EPOCHS: 330 | LOSS: 0.015276751920821096 | ACC: 1.0\n",
      "EPOCHS: 331 | LOSS: 0.015213917821813269 | ACC: 1.0\n",
      "EPOCHS: 332 | LOSS: 0.015151549074880961 | ACC: 1.0\n",
      "EPOCHS: 333 | LOSS: 0.015089640809438688 | ACC: 1.0\n",
      "EPOCHS: 334 | LOSS: 0.015028188219870712 | ACC: 1.0\n",
      "EPOCHS: 335 | LOSS: 0.014967186564488644 | ACC: 1.0\n",
      "EPOCHS: 336 | LOSS: 0.014906631164508868 | ACC: 1.0\n",
      "EPOCHS: 337 | LOSS: 0.014846517403048945 | ACC: 1.0\n",
      "EPOCHS: 338 | LOSS: 0.014786840724142198 | ACC: 1.0\n",
      "EPOCHS: 339 | LOSS: 0.014727596631770867 | ACC: 1.0\n",
      "EPOCHS: 340 | LOSS: 0.014668780688916624 | ACC: 1.0\n",
      "EPOCHS: 341 | LOSS: 0.01461038851662903 | ACC: 1.0\n",
      "EPOCHS: 342 | LOSS: 0.01455241579311054 | ACC: 1.0\n",
      "EPOCHS: 343 | LOSS: 0.014494858252818495 | ACC: 1.0\n",
      "EPOCHS: 344 | LOSS: 0.014437711685583325 | ACC: 1.0\n",
      "EPOCHS: 345 | LOSS: 0.014380971935743107 | ACC: 1.0\n",
      "EPOCHS: 346 | LOSS: 0.014324634901293239 | ACC: 1.0\n",
      "EPOCHS: 347 | LOSS: 0.014268696533051942 | ACC: 1.0\n",
      "EPOCHS: 348 | LOSS: 0.014213152833840836 | ACC: 1.0\n",
      "EPOCHS: 349 | LOSS: 0.01415799985767989 | ACC: 1.0\n",
      "EPOCHS: 350 | LOSS: 0.014103233708997171 | ACC: 1.0\n",
      "EPOCHS: 351 | LOSS: 0.014048850541852667 | ACC: 1.0\n",
      "EPOCHS: 352 | LOSS: 0.013994846559175876 | ACC: 1.0\n",
      "EPOCHS: 353 | LOSS: 0.01394121801201719 | ACC: 1.0\n",
      "EPOCHS: 354 | LOSS: 0.013887961198812287 | ACC: 1.0\n",
      "EPOCHS: 355 | LOSS: 0.013835072464659924 | ACC: 1.0\n",
      "EPOCHS: 356 | LOSS: 0.01378254820061231 | ACC: 1.0\n",
      "EPOCHS: 357 | LOSS: 0.013730384842978068 | ACC: 1.0\n",
      "EPOCHS: 358 | LOSS: 0.013678578872637397 | ACC: 1.0\n",
      "EPOCHS: 359 | LOSS: 0.013627126814369445 | ACC: 1.0\n",
      "EPOCHS: 360 | LOSS: 0.013576025236191272 | ACC: 1.0\n",
      "EPOCHS: 361 | LOSS: 0.013525270748708691 | ACC: 1.0\n",
      "EPOCHS: 362 | LOSS: 0.013474860004477946 | ACC: 1.0\n",
      "EPOCHS: 363 | LOSS: 0.013424789697379098 | ACC: 1.0\n",
      "EPOCHS: 364 | LOSS: 0.013375056561999762 | ACC: 1.0\n",
      "EPOCHS: 365 | LOSS: 0.01332565737302988 | ACC: 1.0\n",
      "EPOCHS: 366 | LOSS: 0.013276588944666904 | ACC: 1.0\n",
      "EPOCHS: 367 | LOSS: 0.013227848130030981 | ACC: 1.0\n",
      "EPOCHS: 368 | LOSS: 0.0131794318205906 | ACC: 1.0\n",
      "EPOCHS: 369 | LOSS: 0.013131336945597928 | ACC: 1.0\n",
      "EPOCHS: 370 | LOSS: 0.013083560471533551 | ACC: 1.0\n",
      "EPOCHS: 371 | LOSS: 0.013036099401561278 | ACC: 1.0\n",
      "EPOCHS: 372 | LOSS: 0.012988950774991693 | ACC: 1.0\n",
      "EPOCHS: 373 | LOSS: 0.012942111666755199 | ACC: 1.0\n",
      "EPOCHS: 374 | LOSS: 0.012895579186883784 | ACC: 1.0\n",
      "EPOCHS: 375 | LOSS: 0.012849350480001718 | ACC: 1.0\n",
      "EPOCHS: 376 | LOSS: 0.012803422724824796 | ACC: 1.0\n",
      "EPOCHS: 377 | LOSS: 0.012757793133667981 | ACC: 1.0\n",
      "EPOCHS: 378 | LOSS: 0.012712458951961339 | ACC: 1.0\n",
      "EPOCHS: 379 | LOSS: 0.01266741745777427 | ACC: 1.0\n",
      "EPOCHS: 380 | LOSS: 0.012622665961347371 | ACC: 1.0\n",
      "EPOCHS: 381 | LOSS: 0.012578201804632509 | ACC: 1.0\n",
      "EPOCHS: 382 | LOSS: 0.012534022360840098 | ACC: 1.0\n",
      "EPOCHS: 383 | LOSS: 0.012490125033994623 | ACC: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 384 | LOSS: 0.012446507258496442 | ACC: 1.0\n",
      "EPOCHS: 385 | LOSS: 0.012403166498692057 | ACC: 1.0\n",
      "EPOCHS: 386 | LOSS: 0.012360100248450625 | ACC: 1.0\n",
      "EPOCHS: 387 | LOSS: 0.01231730603074778 | ACC: 1.0\n",
      "EPOCHS: 388 | LOSS: 0.012274781397256192 | ACC: 1.0\n",
      "EPOCHS: 389 | LOSS: 0.012232523927943044 | ACC: 1.0\n",
      "EPOCHS: 390 | LOSS: 0.012190531230673889 | ACC: 1.0\n",
      "EPOCHS: 391 | LOSS: 0.012148800940822919 | ACC: 1.0\n",
      "EPOCHS: 392 | LOSS: 0.012107330720889792 | ACC: 1.0\n",
      "EPOCHS: 393 | LOSS: 0.012066118260122726 | ACC: 1.0\n",
      "EPOCHS: 394 | LOSS: 0.012025161274147335 | ACC: 1.0\n",
      "EPOCHS: 395 | LOSS: 0.01198445750460187 | ACC: 1.0\n",
      "EPOCHS: 396 | LOSS: 0.01194400471877817 | ACC: 1.0\n",
      "EPOCHS: 397 | LOSS: 0.011903800709268397 | ACC: 1.0\n",
      "EPOCHS: 398 | LOSS: 0.011863843293617475 | ACC: 1.0\n",
      "EPOCHS: 399 | LOSS: 0.011824130313981133 | ACC: 1.0\n",
      "EPOCHS: 400 | LOSS: 0.011784659636789357 | ACC: 1.0\n",
      "EPOCHS: 401 | LOSS: 0.01174542915241529 | ACC: 1.0\n",
      "EPOCHS: 402 | LOSS: 0.011706436774849225 | ACC: 1.0\n",
      "EPOCHS: 403 | LOSS: 0.011667680441378232 | ACC: 1.0\n",
      "EPOCHS: 404 | LOSS: 0.01162915811227016 | ACC: 1.0\n",
      "EPOCHS: 405 | LOSS: 0.011590867770463314 | ACC: 1.0\n",
      "EPOCHS: 406 | LOSS: 0.011552807421260714 | ACC: 1.0\n",
      "EPOCHS: 407 | LOSS: 0.011514975092029037 | ACC: 1.0\n",
      "EPOCHS: 408 | LOSS: 0.011477368831902713 | ACC: 1.0\n",
      "EPOCHS: 409 | LOSS: 0.011439986711492232 | ACC: 1.0\n",
      "EPOCHS: 410 | LOSS: 0.01140282682259724 | ACC: 1.0\n",
      "EPOCHS: 411 | LOSS: 0.011365887277924236 | ACC: 1.0\n",
      "EPOCHS: 412 | LOSS: 0.011329166210808164 | ACC: 1.0\n",
      "EPOCHS: 413 | LOSS: 0.011292661774938994 | ACC: 1.0\n",
      "EPOCHS: 414 | LOSS: 0.011256372144092166 | ACC: 1.0\n",
      "EPOCHS: 415 | LOSS: 0.011220295511863007 | ACC: 1.0\n",
      "EPOCHS: 416 | LOSS: 0.011184430091406026 | ACC: 1.0\n",
      "EPOCHS: 417 | LOSS: 0.01114877411517716 | ACC: 1.0\n",
      "EPOCHS: 418 | LOSS: 0.011113325834680958 | ACC: 1.0\n",
      "EPOCHS: 419 | LOSS: 0.011078083520220909 | ACC: 1.0\n",
      "EPOCHS: 420 | LOSS: 0.011043045460654202 | ACC: 1.0\n",
      "EPOCHS: 421 | LOSS: 0.0110082099631497 | ACC: 1.0\n",
      "EPOCHS: 422 | LOSS: 0.010973575352949981 | ACC: 1.0\n",
      "EPOCHS: 423 | LOSS: 0.010939139973136855 | ACC: 1.0\n",
      "EPOCHS: 424 | LOSS: 0.010904902184400397 | ACC: 1.0\n",
      "EPOCHS: 425 | LOSS: 0.0108708603648115 | ACC: 1.0\n",
      "EPOCHS: 426 | LOSS: 0.010837012909598041 | ACC: 1.0\n",
      "EPOCHS: 427 | LOSS: 0.010803358230924176 | ACC: 1.0\n",
      "EPOCHS: 428 | LOSS: 0.010769894757673055 | ACC: 1.0\n",
      "EPOCHS: 429 | LOSS: 0.01073662093523286 | ACC: 1.0\n",
      "EPOCHS: 430 | LOSS: 0.010703535225285946 | ACC: 1.0\n",
      "EPOCHS: 431 | LOSS: 0.010670636105601192 | ACC: 1.0\n",
      "EPOCHS: 432 | LOSS: 0.010637922069829548 | ACC: 1.0\n",
      "EPOCHS: 433 | LOSS: 0.010605391627302282 | ACC: 1.0\n",
      "EPOCHS: 434 | LOSS: 0.010573043302832808 | ACC: 1.0\n",
      "EPOCHS: 435 | LOSS: 0.010540875636520737 | ACC: 1.0\n",
      "EPOCHS: 436 | LOSS: 0.010508887183559556 | ACC: 1.0\n",
      "EPOCHS: 437 | LOSS: 0.010477076514046457 | ACC: 1.0\n",
      "EPOCHS: 438 | LOSS: 0.010445442212795667 | ACC: 1.0\n",
      "EPOCHS: 439 | LOSS: 0.010413982879153759 | ACC: 1.0\n",
      "EPOCHS: 440 | LOSS: 0.010382697126818418 | ACC: 1.0\n",
      "EPOCHS: 441 | LOSS: 0.01035158358365934 | ACC: 1.0\n",
      "EPOCHS: 442 | LOSS: 0.010320640891541864 | ACC: 1.0\n",
      "EPOCHS: 443 | LOSS: 0.010289867706153557 | ACC: 1.0\n",
      "EPOCHS: 444 | LOSS: 0.010259262696832527 | ACC: 1.0\n",
      "EPOCHS: 445 | LOSS: 0.01022882454639906 | ACC: 1.0\n",
      "EPOCHS: 446 | LOSS: 0.010198551950989204 | ACC: 1.0\n",
      "EPOCHS: 447 | LOSS: 0.01016844361989093 | ACC: 1.0\n",
      "EPOCHS: 448 | LOSS: 0.010138498275382634 | ACC: 1.0\n",
      "EPOCHS: 449 | LOSS: 0.01010871465257379 | ACC: 1.0\n",
      "EPOCHS: 450 | LOSS: 0.010079091499248367 | ACC: 1.0\n",
      "EPOCHS: 451 | LOSS: 0.010049627575709931 | ACC: 1.0\n",
      "EPOCHS: 452 | LOSS: 0.010020321654629186 | ACC: 1.0\n",
      "EPOCHS: 453 | LOSS: 0.009991172520893864 | ACC: 1.0\n",
      "EPOCHS: 454 | LOSS: 0.009962178971460643 | ACC: 1.0\n",
      "EPOCHS: 455 | LOSS: 0.009933339815208905 | ACC: 1.0\n",
      "EPOCHS: 456 | LOSS: 0.009904653872796997 | ACC: 1.0\n",
      "EPOCHS: 457 | LOSS: 0.009876119976520287 | ACC: 1.0\n",
      "EPOCHS: 458 | LOSS: 0.009847736970171203 | ACC: 1.0\n",
      "EPOCHS: 459 | LOSS: 0.009819503708901419 | ACC: 1.0\n",
      "EPOCHS: 460 | LOSS: 0.009791419059085725 | ACC: 1.0\n",
      "EPOCHS: 461 | LOSS: 0.00976348189818814 | ACC: 1.0\n",
      "EPOCHS: 462 | LOSS: 0.009735691114629602 | ACC: 1.0\n",
      "EPOCHS: 463 | LOSS: 0.009708045607657598 | ACC: 1.0\n",
      "EPOCHS: 464 | LOSS: 0.009680544287217848 | ACC: 1.0\n",
      "EPOCHS: 465 | LOSS: 0.009653186073827232 | ACC: 1.0\n",
      "EPOCHS: 466 | LOSS: 0.009625969898449159 | ACC: 1.0\n",
      "EPOCHS: 467 | LOSS: 0.00959889470237016 | ACC: 1.0\n",
      "EPOCHS: 468 | LOSS: 0.009571959437078378 | ACC: 1.0\n",
      "EPOCHS: 469 | LOSS: 0.009545163064143701 | ACC: 1.0\n",
      "EPOCHS: 470 | LOSS: 0.009518504555099677 | ACC: 1.0\n",
      "EPOCHS: 471 | LOSS: 0.009491982891326878 | ACC: 1.0\n",
      "EPOCHS: 472 | LOSS: 0.009465597063937816 | ACC: 1.0\n",
      "EPOCHS: 473 | LOSS: 0.009439346073663772 | ACC: 1.0\n",
      "EPOCHS: 474 | LOSS: 0.00941322893074279 | ACC: 1.0\n",
      "EPOCHS: 475 | LOSS: 0.00938724465480947 | ACC: 1.0\n",
      "EPOCHS: 476 | LOSS: 0.009361392274786141 | ACC: 1.0\n",
      "EPOCHS: 477 | LOSS: 0.00933567082877552 | ACC: 1.0\n",
      "EPOCHS: 478 | LOSS: 0.009310079363954699 | ACC: 1.0\n",
      "EPOCHS: 479 | LOSS: 0.009284616936471006 | ACC: 1.0\n",
      "EPOCHS: 480 | LOSS: 0.009259282611338774 | ACC: 1.0\n",
      "EPOCHS: 481 | LOSS: 0.00923407546233764 | ACC: 1.0\n",
      "EPOCHS: 482 | LOSS: 0.00920899457191262 | ACC: 1.0\n",
      "EPOCHS: 483 | LOSS: 0.009184039031074754 | ACC: 1.0\n",
      "EPOCHS: 484 | LOSS: 0.009159207939303856 | ACC: 1.0\n",
      "EPOCHS: 485 | LOSS: 0.009134500404452083 | ACC: 1.0\n",
      "EPOCHS: 486 | LOSS: 0.00910991554264892 | ACC: 1.0\n",
      "EPOCHS: 487 | LOSS: 0.009085452478207474 | ACC: 1.0\n",
      "EPOCHS: 488 | LOSS: 0.009061110343532052 | ACC: 1.0\n",
      "EPOCHS: 489 | LOSS: 0.009036888279026897 | ACC: 1.0\n",
      "EPOCHS: 490 | LOSS: 0.009012785433006012 | ACC: 1.0\n",
      "EPOCHS: 491 | LOSS: 0.00898880096160444 | ACC: 1.0\n",
      "EPOCHS: 492 | LOSS: 0.00896493402869055 | ACC: 1.0\n",
      "EPOCHS: 493 | LOSS: 0.008941183805779579 | ACC: 1.0\n",
      "EPOCHS: 494 | LOSS: 0.008917549471948047 | ACC: 1.0\n",
      "EPOCHS: 495 | LOSS: 0.00889403021374967 | ACC: 1.0\n",
      "EPOCHS: 496 | LOSS: 0.00887062522513223 | ACC: 1.0\n",
      "EPOCHS: 497 | LOSS: 0.008847333707355359 | ACC: 1.0\n",
      "EPOCHS: 498 | LOSS: 0.008824154868909589 | ACC: 1.0\n",
      "EPOCHS: 499 | LOSS: 0.008801087925436457 | ACC: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = NNBP(epochs=500)\n",
    "loss_lst1, acc_lst1 = model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee8396",
   "metadata": {},
   "source": [
    "### 누적 BP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b165b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBP:\n",
    "    def __init__(self, lr=0.1, epochs=100, n_hidden_layer_node=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_hidden_layer_node = n_hidden_layer_node\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def update(self, x_, y_):\n",
    "        x2 = self.sigmoid(np.dot(x_, self.w1) + self.b1) # n x n_hidden_layer_node\n",
    "        y2 = self.sigmoid(np.dot(x2, self.w2) + self.b2) # n x n_output_node\n",
    "        \n",
    "        y2y_diff = y2 - y_\n",
    "        grad_b1_ = np.dot(y2y_diff, self.w2.T) * x2 * (1 - x2)\n",
    "        \n",
    "        grad_b1 = np.sum(grad_b1_, axis=0)\n",
    "        grad_w1 = np.dot(x_.T, grad_b1_)\n",
    "        grad_b2 = np.sum(y2y_diff, axis=0)\n",
    "        grad_w2 = np.dot(x2.T , y2y_diff)\n",
    "        \n",
    "        self.b1 -= self.lr * grad_b1\n",
    "        self.w1 -= self.lr * grad_w1\n",
    "        self.b2 -= self.lr * grad_b2\n",
    "        self.w2 -= self.lr * grad_w2\n",
    "        \n",
    "    def crossentropy(self, y_hat, y_):\n",
    "        return np.mean(-y_ * np.log(y_hat) - (1 - y_) * np.log(1 - y_hat))\n",
    "    \n",
    "    def accuracy(self, y_hat, y_):\n",
    "        return np.mean((y_hat >= 0.5).astype(np.int) == y_)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        n_input_node = x.shape[-1]\n",
    "        n_output_node = y.shape[-1]\n",
    "        \n",
    "        self.length = len(x)\n",
    "        \n",
    "        self.w1 = np.random.randn(n_input_node, self.n_hidden_layer_node)\n",
    "        self.b1 = np.zeros((1, self.n_hidden_layer_node))\n",
    "        \n",
    "        self.w2 = np.random.randn(self.n_hidden_layer_node, n_output_node)\n",
    "        self.b2 = np.zeros((1, n_output_node))\n",
    "        \n",
    "        loss_lst = []\n",
    "        acc_lst = []\n",
    "        \n",
    "        for j in range(self.epochs):\n",
    "            self.update(x, y)\n",
    "\n",
    "            pred = self.predict(x)\n",
    "            loss = self.crossentropy(pred, y)\n",
    "            acc = self.accuracy(pred, y)\n",
    "            \n",
    "            loss_lst += [loss]\n",
    "            acc_lst += [acc]\n",
    "            \n",
    "            print(\"EPOCHS: {} | LOSS: {} | ACC: {}\".format(j, loss, acc))\n",
    "            \n",
    "        return loss_lst, acc_lst\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.sigmoid(np.dot(self.sigmoid(np.dot(x, self.w1) + self.b1), self.w2) + self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5c0dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinjy\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 0 | LOSS: 0.8935726883295186 | ACC: 0.5294117647058824\n",
      "EPOCHS: 1 | LOSS: 0.8224108139617688 | ACC: 0.47058823529411764\n",
      "EPOCHS: 2 | LOSS: 0.7654531440419249 | ACC: 0.5294117647058824\n",
      "EPOCHS: 3 | LOSS: 0.7181346317864636 | ACC: 0.5294117647058824\n",
      "EPOCHS: 4 | LOSS: 0.6747161086740758 | ACC: 0.6470588235294118\n",
      "EPOCHS: 5 | LOSS: 0.6303172405608805 | ACC: 0.5882352941176471\n",
      "EPOCHS: 6 | LOSS: 0.5887830248495414 | ACC: 0.6470588235294118\n",
      "EPOCHS: 7 | LOSS: 0.5437246571193423 | ACC: 0.7058823529411765\n",
      "EPOCHS: 8 | LOSS: 0.5042005245744591 | ACC: 0.7647058823529411\n",
      "EPOCHS: 9 | LOSS: 0.46386504243626664 | ACC: 0.6470588235294118\n",
      "EPOCHS: 10 | LOSS: 0.4309558109198205 | ACC: 0.8235294117647058\n",
      "EPOCHS: 11 | LOSS: 0.40105601179407485 | ACC: 0.8823529411764706\n",
      "EPOCHS: 12 | LOSS: 0.3775081585537999 | ACC: 0.8823529411764706\n",
      "EPOCHS: 13 | LOSS: 0.35788349270959724 | ACC: 0.8823529411764706\n",
      "EPOCHS: 14 | LOSS: 0.3420677447362203 | ACC: 0.8823529411764706\n",
      "EPOCHS: 15 | LOSS: 0.3288364156754604 | ACC: 0.8823529411764706\n",
      "EPOCHS: 16 | LOSS: 0.3175651770738913 | ACC: 0.8823529411764706\n",
      "EPOCHS: 17 | LOSS: 0.30774800941617847 | ACC: 0.8823529411764706\n",
      "EPOCHS: 18 | LOSS: 0.2990799006176653 | ACC: 0.8823529411764706\n",
      "EPOCHS: 19 | LOSS: 0.2913549028814413 | ACC: 0.8823529411764706\n",
      "EPOCHS: 20 | LOSS: 0.28442278286390055 | ACC: 0.8823529411764706\n",
      "EPOCHS: 21 | LOSS: 0.27816444477896674 | ACC: 0.8823529411764706\n",
      "EPOCHS: 22 | LOSS: 0.2724814907782466 | ACC: 0.8823529411764706\n",
      "EPOCHS: 23 | LOSS: 0.2672915194156034 | ACC: 0.8823529411764706\n",
      "EPOCHS: 24 | LOSS: 0.2625252531147212 | ACC: 0.8823529411764706\n",
      "EPOCHS: 25 | LOSS: 0.2581243360304356 | ACC: 0.8823529411764706\n",
      "EPOCHS: 26 | LOSS: 0.2540394882909344 | ACC: 0.8823529411764706\n",
      "EPOCHS: 27 | LOSS: 0.2502289571479835 | ACC: 0.8823529411764706\n",
      "EPOCHS: 28 | LOSS: 0.24665723033712114 | ACC: 0.8823529411764706\n",
      "EPOCHS: 29 | LOSS: 0.2432939775572775 | ACC: 0.8823529411764706\n",
      "EPOCHS: 30 | LOSS: 0.2401131846593372 | ACC: 0.8823529411764706\n",
      "EPOCHS: 31 | LOSS: 0.23709244803416213 | ACC: 0.8823529411764706\n",
      "EPOCHS: 32 | LOSS: 0.23421240109340263 | ACC: 0.8823529411764706\n",
      "EPOCHS: 33 | LOSS: 0.23145624938349688 | ACC: 0.8823529411764706\n",
      "EPOCHS: 34 | LOSS: 0.22880939504872444 | ACC: 0.8823529411764706\n",
      "EPOCHS: 35 | LOSS: 0.22625913479287024 | ACC: 0.8823529411764706\n",
      "EPOCHS: 36 | LOSS: 0.2237944181431472 | ACC: 0.8823529411764706\n",
      "EPOCHS: 37 | LOSS: 0.22140565476072516 | ACC: 0.8823529411764706\n",
      "EPOCHS: 38 | LOSS: 0.21908456089402328 | ACC: 0.8823529411764706\n",
      "EPOCHS: 39 | LOSS: 0.21682403599221525 | ACC: 0.8823529411764706\n",
      "EPOCHS: 40 | LOSS: 0.2146180611694048 | ACC: 0.8823529411764706\n",
      "EPOCHS: 41 | LOSS: 0.21246161182905537 | ACC: 0.8823529411764706\n",
      "EPOCHS: 42 | LOSS: 0.21035057751037722 | ACC: 0.8823529411764706\n",
      "EPOCHS: 43 | LOSS: 0.20828168305360392 | ACC: 0.8823529411764706\n",
      "EPOCHS: 44 | LOSS: 0.2062524065783223 | ACC: 0.8823529411764706\n",
      "EPOCHS: 45 | LOSS: 0.20426089150694857 | ACC: 0.8823529411764706\n",
      "EPOCHS: 46 | LOSS: 0.20230585181187458 | ACC: 0.8823529411764706\n",
      "EPOCHS: 47 | LOSS: 0.20038647159508932 | ACC: 0.8823529411764706\n",
      "EPOCHS: 48 | LOSS: 0.19850230175480235 | ACC: 0.8823529411764706\n",
      "EPOCHS: 49 | LOSS: 0.19665315761093216 | ACC: 0.8823529411764706\n",
      "EPOCHS: 50 | LOSS: 0.19483902179759055 | ACC: 0.8823529411764706\n",
      "EPOCHS: 51 | LOSS: 0.1930599564662999 | ACC: 0.8823529411764706\n",
      "EPOCHS: 52 | LOSS: 0.191316027994444 | ACC: 0.8823529411764706\n",
      "EPOCHS: 53 | LOSS: 0.18960724617407004 | ACC: 0.8823529411764706\n",
      "EPOCHS: 54 | LOSS: 0.1879335185197338 | ACC: 0.8823529411764706\n",
      "EPOCHS: 55 | LOSS: 0.18629461911001735 | ACC: 0.8823529411764706\n",
      "EPOCHS: 56 | LOSS: 0.18469017042682506 | ACC: 0.8823529411764706\n",
      "EPOCHS: 57 | LOSS: 0.18311963605490864 | ACC: 0.8823529411764706\n",
      "EPOCHS: 58 | LOSS: 0.18158232184824935 | ACC: 0.8823529411764706\n",
      "EPOCHS: 59 | LOSS: 0.1800773832019577 | ACC: 0.8823529411764706\n",
      "EPOCHS: 60 | LOSS: 0.17860383630342477 | ACC: 0.8823529411764706\n",
      "EPOCHS: 61 | LOSS: 0.17716057158720805 | ACC: 0.8823529411764706\n",
      "EPOCHS: 62 | LOSS: 0.1757463680105186 | ACC: 0.8823529411764706\n",
      "EPOCHS: 63 | LOSS: 0.17435990714677518 | ACC: 0.9411764705882353\n",
      "EPOCHS: 64 | LOSS: 0.17299978643116737 | ACC: 0.9411764705882353\n",
      "EPOCHS: 65 | LOSS: 0.17166453116976965 | ACC: 0.9411764705882353\n",
      "EPOCHS: 66 | LOSS: 0.17035260514029768 | ACC: 0.9411764705882353\n",
      "EPOCHS: 67 | LOSS: 0.16906241977355185 | ACC: 0.9411764705882353\n",
      "EPOCHS: 68 | LOSS: 0.16779234201909846 | ACC: 0.9411764705882353\n",
      "EPOCHS: 69 | LOSS: 0.16654070107684743 | ACC: 0.9411764705882353\n",
      "EPOCHS: 70 | LOSS: 0.16530579422720995 | ACC: 0.9411764705882353\n",
      "EPOCHS: 71 | LOSS: 0.16408589202413745 | ACC: 0.9411764705882353\n",
      "EPOCHS: 72 | LOSS: 0.16287924313333152 | ACC: 0.9411764705882353\n",
      "EPOCHS: 73 | LOSS: 0.1616840791061281 | ACC: 0.9411764705882353\n",
      "EPOCHS: 74 | LOSS: 0.16049861938016188 | ACC: 0.9411764705882353\n",
      "EPOCHS: 75 | LOSS: 0.15932107679155694 | ACC: 0.9411764705882353\n",
      "EPOCHS: 76 | LOSS: 0.1581496638695689 | ACC: 0.9411764705882353\n",
      "EPOCHS: 77 | LOSS: 0.15698260016187957 | ACC: 0.9411764705882353\n",
      "EPOCHS: 78 | LOSS: 0.15581812080512525 | ACC: 0.9411764705882353\n",
      "EPOCHS: 79 | LOSS: 0.15465448650852825 | ACC: 0.9411764705882353\n",
      "EPOCHS: 80 | LOSS: 0.15348999505674332 | ACC: 0.9411764705882353\n",
      "EPOCHS: 81 | LOSS: 0.15232299436005847 | ACC: 0.9411764705882353\n",
      "EPOCHS: 82 | LOSS: 0.15115189698606987 | ACC: 0.9411764705882353\n",
      "EPOCHS: 83 | LOSS: 0.1499751959989493 | ACC: 0.9411764705882353\n",
      "EPOCHS: 84 | LOSS: 0.14879148181488894 | ACC: 0.9411764705882353\n",
      "EPOCHS: 85 | LOSS: 0.147599459662259 | ACC: 0.9411764705882353\n",
      "EPOCHS: 86 | LOSS: 0.14639796712190817 | ACC: 0.9411764705882353\n",
      "EPOCHS: 87 | LOSS: 0.1451859911281309 | ACC: 0.9411764705882353\n",
      "EPOCHS: 88 | LOSS: 0.14396268374597154 | ACC: 0.9411764705882353\n",
      "EPOCHS: 89 | LOSS: 0.1427273760165478 | ACC: 0.9411764705882353\n",
      "EPOCHS: 90 | LOSS: 0.14147958918684675 | ACC: 0.9411764705882353\n",
      "EPOCHS: 91 | LOSS: 0.14021904271716107 | ACC: 0.9411764705882353\n",
      "EPOCHS: 92 | LOSS: 0.1389456585852769 | ACC: 0.9411764705882353\n",
      "EPOCHS: 93 | LOSS: 0.1376595615727968 | ACC: 0.9411764705882353\n",
      "EPOCHS: 94 | LOSS: 0.13636107541140385 | ACC: 0.9411764705882353\n",
      "EPOCHS: 95 | LOSS: 0.13505071486794065 | ACC: 0.9411764705882353\n",
      "EPOCHS: 96 | LOSS: 0.13372917403874685 | ACC: 0.9411764705882353\n",
      "EPOCHS: 97 | LOSS: 0.13239731128975718 | ACC: 0.9411764705882353\n",
      "EPOCHS: 98 | LOSS: 0.13105613140771194 | ACC: 0.9411764705882353\n",
      "EPOCHS: 99 | LOSS: 0.12970676561330316 | ACC: 0.9411764705882353\n",
      "EPOCHS: 100 | LOSS: 0.1283504501284447 | ACC: 0.9411764705882353\n",
      "EPOCHS: 101 | LOSS: 0.12698850399080397 | ACC: 0.9411764705882353\n",
      "EPOCHS: 102 | LOSS: 0.1256223067757527 | ACC: 0.9411764705882353\n",
      "EPOCHS: 103 | LOSS: 0.12425327682661527 | ACC: 0.9411764705882353\n",
      "EPOCHS: 104 | LOSS: 0.12288285051602721 | ACC: 0.9411764705882353\n",
      "EPOCHS: 105 | LOSS: 0.12151246297107049 | ACC: 0.9411764705882353\n",
      "EPOCHS: 106 | LOSS: 0.120143530598356 | ACC: 0.9411764705882353\n",
      "EPOCHS: 107 | LOSS: 0.11877743564726026 | ACC: 0.9411764705882353\n",
      "EPOCHS: 108 | LOSS: 0.11741551295437178 | ACC: 0.9411764705882353\n",
      "EPOCHS: 109 | LOSS: 0.11605903892362589 | ACC: 0.9411764705882353\n",
      "EPOCHS: 110 | LOSS: 0.11470922271782269 | ACC: 0.9411764705882353\n",
      "EPOCHS: 111 | LOSS: 0.11336719957073581 | ACC: 0.9411764705882353\n",
      "EPOCHS: 112 | LOSS: 0.11203402607641565 | ACC: 0.9411764705882353\n",
      "EPOCHS: 113 | LOSS: 0.11071067727414335 | ACC: 0.9411764705882353\n",
      "EPOCHS: 114 | LOSS: 0.10939804532335247 | ACC: 0.9411764705882353\n",
      "EPOCHS: 115 | LOSS: 0.10809693955139263 | ACC: 0.9411764705882353\n",
      "EPOCHS: 116 | LOSS: 0.10680808765633118 | ACC: 0.9411764705882353\n",
      "EPOCHS: 117 | LOSS: 0.10553213785481606 | ACC: 0.9411764705882353\n",
      "EPOCHS: 118 | LOSS: 0.10426966177902741 | ACC: 0.9411764705882353\n",
      "EPOCHS: 119 | LOSS: 0.10302115794477312 | ACC: 0.9411764705882353\n",
      "EPOCHS: 120 | LOSS: 0.10178705563297873 | ACC: 0.9411764705882353\n",
      "EPOCHS: 121 | LOSS: 0.10056771904770001 | ACC: 0.9411764705882353\n",
      "EPOCHS: 122 | LOSS: 0.09936345163426201 | ACC: 0.9411764705882353\n",
      "EPOCHS: 123 | LOSS: 0.09817450046044512 | ACC: 0.9411764705882353\n",
      "EPOCHS: 124 | LOSS: 0.09700106058134837 | ACC: 1.0\n",
      "EPOCHS: 125 | LOSS: 0.09584327932442738 | ACC: 1.0\n",
      "EPOCHS: 126 | LOSS: 0.0947012604451669 | ACC: 1.0\n",
      "EPOCHS: 127 | LOSS: 0.09357506811593613 | ACC: 1.0\n",
      "EPOCHS: 128 | LOSS: 0.09246473072090619 | ACC: 1.0\n",
      "EPOCHS: 129 | LOSS: 0.09137024443862092 | ACC: 1.0\n",
      "EPOCHS: 130 | LOSS: 0.09029157660105946 | ACC: 1.0\n",
      "EPOCHS: 131 | LOSS: 0.08922866882398502 | ACC: 1.0\n",
      "EPOCHS: 132 | LOSS: 0.08818143990818102 | ACC: 1.0\n",
      "EPOCHS: 133 | LOSS: 0.08714978851499249 | ACC: 1.0\n",
      "EPOCHS: 134 | LOSS: 0.08613359562254523 | ACC: 1.0\n",
      "EPOCHS: 135 | LOSS: 0.08513272677123847 | ACC: 1.0\n",
      "EPOCHS: 136 | LOSS: 0.08414703410870386 | ACC: 1.0\n",
      "EPOCHS: 137 | LOSS: 0.08317635824550511 | ACC: 1.0\n",
      "EPOCHS: 138 | LOSS: 0.08222052993350101 | ACC: 1.0\n",
      "EPOCHS: 139 | LOSS: 0.08127937157909143 | ACC: 1.0\n",
      "EPOCHS: 140 | LOSS: 0.08035269860358694 | ACC: 1.0\n",
      "EPOCHS: 141 | LOSS: 0.07944032066273865 | ACC: 1.0\n",
      "EPOCHS: 142 | LOSS: 0.07854204273709722 | ACC: 1.0\n",
      "EPOCHS: 143 | LOSS: 0.07765766610437831 | ACC: 1.0\n",
      "EPOCHS: 144 | LOSS: 0.07678698920443183 | ACC: 1.0\n",
      "EPOCHS: 145 | LOSS: 0.07592980840677892 | ACC: 1.0\n",
      "EPOCHS: 146 | LOSS: 0.0750859186900167 | ACC: 1.0\n",
      "EPOCHS: 147 | LOSS: 0.07425511424171298 | ACC: 1.0\n",
      "EPOCHS: 148 | LOSS: 0.07343718898674265 | ACC: 1.0\n",
      "EPOCHS: 149 | LOSS: 0.07263193705136266 | ACC: 1.0\n",
      "EPOCHS: 150 | LOSS: 0.07183915316969038 | ACC: 1.0\n",
      "EPOCHS: 151 | LOSS: 0.07105863303865276 | ACC: 1.0\n",
      "EPOCHS: 152 | LOSS: 0.07029017362690797 | ACC: 1.0\n",
      "EPOCHS: 153 | LOSS: 0.06953357344271428 | ACC: 1.0\n",
      "EPOCHS: 154 | LOSS: 0.06878863276523418 | ACC: 1.0\n",
      "EPOCHS: 155 | LOSS: 0.06805515384330735 | ACC: 1.0\n",
      "EPOCHS: 156 | LOSS: 0.06733294106531808 | ACC: 1.0\n",
      "EPOCHS: 157 | LOSS: 0.06662180110340211 | ACC: 1.0\n",
      "EPOCHS: 158 | LOSS: 0.06592154303489836 | ACC: 1.0\n",
      "EPOCHS: 159 | LOSS: 0.06523197844363783 | ACC: 1.0\n",
      "EPOCHS: 160 | LOSS: 0.06455292150338592 | ACC: 1.0\n",
      "EPOCHS: 161 | LOSS: 0.0638841890454989 | ACC: 1.0\n",
      "EPOCHS: 162 | LOSS: 0.0632256006126312 | ACC: 1.0\n",
      "EPOCHS: 163 | LOSS: 0.06257697850012704 | ACC: 1.0\n",
      "EPOCHS: 164 | LOSS: 0.06193814778654816 | ACC: 1.0\n",
      "EPOCHS: 165 | LOSS: 0.061308936354627516 | ACC: 1.0\n",
      "EPOCHS: 166 | LOSS: 0.060689174903795634 | ACC: 1.0\n",
      "EPOCHS: 167 | LOSS: 0.06007869695529672 | ACC: 1.0\n",
      "EPOCHS: 168 | LOSS: 0.05947733885079724 | ACC: 1.0\n",
      "EPOCHS: 169 | LOSS: 0.05888493974528976 | ACC: 1.0\n",
      "EPOCHS: 170 | LOSS: 0.05830134159500216 | ACC: 1.0\n",
      "EPOCHS: 171 | LOSS: 0.05772638914094541 | ACC: 1.0\n",
      "EPOCHS: 172 | LOSS: 0.057159929888659505 | ACC: 1.0\n",
      "EPOCHS: 173 | LOSS: 0.05660181408465541 | ACC: 1.0\n",
      "EPOCHS: 174 | LOSS: 0.05605189468999638 | ACC: 1.0\n",
      "EPOCHS: 175 | LOSS: 0.055510027351409336 | ACC: 1.0\n",
      "EPOCHS: 176 | LOSS: 0.05497607037027765 | ACC: 1.0\n",
      "EPOCHS: 177 | LOSS: 0.05444988466982355 | ACC: 1.0\n",
      "EPOCHS: 178 | LOSS: 0.05393133376075805 | ACC: 1.0\n",
      "EPOCHS: 179 | LOSS: 0.05342028370564154 | ACC: 1.0\n",
      "EPOCHS: 180 | LOSS: 0.05291660308217582 | ACC: 1.0\n",
      "EPOCHS: 181 | LOSS: 0.05242016294562066 | ACC: 1.0\n",
      "EPOCHS: 182 | LOSS: 0.05193083679050795 | ACC: 1.0\n",
      "EPOCHS: 183 | LOSS: 0.05144850051180864 | ACC: 1.0\n",
      "EPOCHS: 184 | LOSS: 0.05097303236568893 | ACC: 1.0\n",
      "EPOCHS: 185 | LOSS: 0.050504312929978555 | ACC: 1.0\n",
      "EPOCHS: 186 | LOSS: 0.05004222506446027 | ACC: 1.0\n",
      "EPOCHS: 187 | LOSS: 0.04958665387107761 | ACC: 1.0\n",
      "EPOCHS: 188 | LOSS: 0.049137486654146766 | ACC: 1.0\n",
      "EPOCHS: 189 | LOSS: 0.04869461288065114 | ACC: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 190 | LOSS: 0.048257924140684985 | ACC: 1.0\n",
      "EPOCHS: 191 | LOSS: 0.047827314108108504 | ACC: 1.0\n",
      "EPOCHS: 192 | LOSS: 0.04740267850146795 | ACC: 1.0\n",
      "EPOCHS: 193 | LOSS: 0.04698391504522765 | ACC: 1.0\n",
      "EPOCHS: 194 | LOSS: 0.046570923431357464 | ACC: 1.0\n",
      "EPOCHS: 195 | LOSS: 0.04616360528131196 | ACC: 1.0\n",
      "EPOCHS: 196 | LOSS: 0.04576186410843386 | ACC: 1.0\n",
      "EPOCHS: 197 | LOSS: 0.04536560528081126 | ACC: 1.0\n",
      "EPOCHS: 198 | LOSS: 0.0449747359846123 | ACC: 1.0\n",
      "EPOCHS: 199 | LOSS: 0.04458916518791964 | ACC: 1.0\n",
      "EPOCHS: 200 | LOSS: 0.04420880360508237 | ACC: 1.0\n",
      "EPOCHS: 201 | LOSS: 0.043833563661602175 | ACC: 1.0\n",
      "EPOCHS: 202 | LOSS: 0.04346335945956523 | ACC: 1.0\n",
      "EPOCHS: 203 | LOSS: 0.04309810674363232 | ACC: 1.0\n",
      "EPOCHS: 204 | LOSS: 0.04273772286759448 | ACC: 1.0\n",
      "EPOCHS: 205 | LOSS: 0.04238212676150228 | ACC: 1.0\n",
      "EPOCHS: 206 | LOSS: 0.04203123889937275 | ACC: 1.0\n",
      "EPOCHS: 207 | LOSS: 0.041684981267478126 | ACC: 1.0\n",
      "EPOCHS: 208 | LOSS: 0.04134327733321798 | ACC: 1.0\n",
      "EPOCHS: 209 | LOSS: 0.04100605201457617 | ACC: 1.0\n",
      "EPOCHS: 210 | LOSS: 0.040673231650161325 | ACC: 1.0\n",
      "EPOCHS: 211 | LOSS: 0.04034474396983012 | ACC: 1.0\n",
      "EPOCHS: 212 | LOSS: 0.04002051806589007 | ACC: 1.0\n",
      "EPOCHS: 213 | LOSS: 0.03970048436487933 | ACC: 1.0\n",
      "EPOCHS: 214 | LOSS: 0.03938457459991826 | ACC: 1.0\n",
      "EPOCHS: 215 | LOSS: 0.039072721783628674 | ACC: 1.0\n",
      "EPOCHS: 216 | LOSS: 0.03876486018161499 | ACC: 1.0\n",
      "EPOCHS: 217 | LOSS: 0.03846092528650042 | ACC: 1.0\n",
      "EPOCHS: 218 | LOSS: 0.03816085379251251 | ACC: 1.0\n",
      "EPOCHS: 219 | LOSS: 0.037864583570610316 | ACC: 1.0\n",
      "EPOCHS: 220 | LOSS: 0.03757205364414575 | ACC: 1.0\n",
      "EPOCHS: 221 | LOSS: 0.03728320416505108 | ACC: 1.0\n",
      "EPOCHS: 222 | LOSS: 0.03699797639054449 | ACC: 1.0\n",
      "EPOCHS: 223 | LOSS: 0.03671631266034519 | ACC: 1.0\n",
      "EPOCHS: 224 | LOSS: 0.03643815637438957 | ACC: 1.0\n",
      "EPOCHS: 225 | LOSS: 0.03616345197103871 | ACC: 1.0\n",
      "EPOCHS: 226 | LOSS: 0.03589214490576937 | ACC: 1.0\n",
      "EPOCHS: 227 | LOSS: 0.03562418163033829 | ACC: 1.0\n",
      "EPOCHS: 228 | LOSS: 0.03535950957241133 | ACC: 1.0\n",
      "EPOCHS: 229 | LOSS: 0.035098077115647205 | ACC: 1.0\n",
      "EPOCHS: 230 | LOSS: 0.0348398335802277 | ACC: 1.0\n",
      "EPOCHS: 231 | LOSS: 0.034584729203823994 | ACC: 1.0\n",
      "EPOCHS: 232 | LOSS: 0.03433271512299013 | ACC: 1.0\n",
      "EPOCHS: 233 | LOSS: 0.03408374335497446 | ACC: 1.0\n",
      "EPOCHS: 234 | LOSS: 0.03383776677993973 | ACC: 1.0\n",
      "EPOCHS: 235 | LOSS: 0.033594739123582615 | ACC: 1.0\n",
      "EPOCHS: 236 | LOSS: 0.033354614940143294 | ACC: 1.0\n",
      "EPOCHS: 237 | LOSS: 0.03311734959579679 | ACC: 1.0\n",
      "EPOCHS: 238 | LOSS: 0.032882899252416636 | ACC: 1.0\n",
      "EPOCHS: 239 | LOSS: 0.03265122085170196 | ACC: 1.0\n",
      "EPOCHS: 240 | LOSS: 0.032422272099660115 | ACC: 1.0\n",
      "EPOCHS: 241 | LOSS: 0.032196011451435386 | ACC: 1.0\n",
      "EPOCHS: 242 | LOSS: 0.03197239809647622 | ACC: 1.0\n",
      "EPOCHS: 243 | LOSS: 0.031751391944032155 | ACC: 1.0\n",
      "EPOCHS: 244 | LOSS: 0.03153295360897287 | ACC: 1.0\n",
      "EPOCHS: 245 | LOSS: 0.03131704439792117 | ACC: 1.0\n",
      "EPOCHS: 246 | LOSS: 0.031103626295692292 | ACC: 1.0\n",
      "EPOCHS: 247 | LOSS: 0.030892661952031886 | ACC: 1.0\n",
      "EPOCHS: 248 | LOSS: 0.030684114668645263 | ACC: 1.0\n",
      "EPOCHS: 249 | LOSS: 0.030477948386510818 | ACC: 1.0\n",
      "EPOCHS: 250 | LOSS: 0.030274127673470576 | ACC: 1.0\n",
      "EPOCHS: 251 | LOSS: 0.030072617712090428 | ACC: 1.0\n",
      "EPOCHS: 252 | LOSS: 0.029873384287784337 | ACC: 1.0\n",
      "EPOCHS: 253 | LOSS: 0.029676393777194873 | ACC: 1.0\n",
      "EPOCHS: 254 | LOSS: 0.02948161313682464 | ACC: 1.0\n",
      "EPOCHS: 255 | LOSS: 0.02928900989191144 | ACC: 1.0\n",
      "EPOCHS: 256 | LOSS: 0.029098552125542115 | ACC: 1.0\n",
      "EPOCHS: 257 | LOSS: 0.028910208467998126 | ACC: 1.0\n",
      "EPOCHS: 258 | LOSS: 0.028723948086328197 | ACC: 1.0\n",
      "EPOCHS: 259 | LOSS: 0.02853974067414136 | ACC: 1.0\n",
      "EPOCHS: 260 | LOSS: 0.028357556441615944 | ACC: 1.0\n",
      "EPOCHS: 261 | LOSS: 0.02817736610571843 | ACC: 1.0\n",
      "EPOCHS: 262 | LOSS: 0.02799914088062768 | ACC: 1.0\n",
      "EPOCHS: 263 | LOSS: 0.027822852468359135 | ACC: 1.0\n",
      "EPOCHS: 264 | LOSS: 0.02764847304958415 | ACC: 1.0\n",
      "EPOCHS: 265 | LOSS: 0.027475975274640144 | ACC: 1.0\n",
      "EPOCHS: 266 | LOSS: 0.027305332254726443 | ACC: 1.0\n",
      "EPOCHS: 267 | LOSS: 0.027136517553281888 | ACC: 1.0\n",
      "EPOCHS: 268 | LOSS: 0.026969505177539224 | ACC: 1.0\n",
      "EPOCHS: 269 | LOSS: 0.026804269570253187 | ACC: 1.0\n",
      "EPOCHS: 270 | LOSS: 0.026640785601596696 | ACC: 1.0\n",
      "EPOCHS: 271 | LOSS: 0.026479028561222532 | ACC: 1.0\n",
      "EPOCHS: 272 | LOSS: 0.02631897415048619 | ACC: 1.0\n",
      "EPOCHS: 273 | LOSS: 0.026160598474825547 | ACC: 1.0\n",
      "EPOCHS: 274 | LOSS: 0.02600387803629496 | ACC: 1.0\n",
      "EPOCHS: 275 | LOSS: 0.02584878972624905 | ACC: 1.0\n",
      "EPOCHS: 276 | LOSS: 0.0256953108181736 | ACC: 1.0\n",
      "EPOCHS: 277 | LOSS: 0.025543418960659696 | ACC: 1.0\n",
      "EPOCHS: 278 | LOSS: 0.025393092170518144 | ACC: 1.0\n",
      "EPOCHS: 279 | LOSS: 0.025244308826031098 | ACC: 1.0\n",
      "EPOCHS: 280 | LOSS: 0.025097047660337327 | ACC: 1.0\n",
      "EPOCHS: 281 | LOSS: 0.02495128775494887 | ACC: 1.0\n",
      "EPOCHS: 282 | LOSS: 0.024807008533395597 | ACC: 1.0\n",
      "EPOCHS: 283 | LOSS: 0.024664189754995076 | ACC: 1.0\n",
      "EPOCHS: 284 | LOSS: 0.02452281150874513 | ACC: 1.0\n",
      "EPOCHS: 285 | LOSS: 0.024382854207336074 | ACC: 1.0\n",
      "EPOCHS: 286 | LOSS: 0.024244298581280525 | ACC: 1.0\n",
      "EPOCHS: 287 | LOSS: 0.02410712567315768 | ACC: 1.0\n",
      "EPOCHS: 288 | LOSS: 0.02397131683196993 | ACC: 1.0\n",
      "EPOCHS: 289 | LOSS: 0.023836853707609604 | ACC: 1.0\n",
      "EPOCHS: 290 | LOSS: 0.02370371824543313 | ACC: 1.0\n",
      "EPOCHS: 291 | LOSS: 0.023571892680940277 | ACC: 1.0\n",
      "EPOCHS: 292 | LOSS: 0.023441359534556776 | ACC: 1.0\n",
      "EPOCHS: 293 | LOSS: 0.023312101606517668 | ACC: 1.0\n",
      "EPOCHS: 294 | LOSS: 0.02318410197184938 | ACC: 1.0\n",
      "EPOCHS: 295 | LOSS: 0.023057343975448757 | ACC: 1.0\n",
      "EPOCHS: 296 | LOSS: 0.02293181122725648 | ACC: 1.0\n",
      "EPOCHS: 297 | LOSS: 0.022807487597523653 | ACC: 1.0\n",
      "EPOCHS: 298 | LOSS: 0.022684357212168722 | ACC: 1.0\n",
      "EPOCHS: 299 | LOSS: 0.022562404448223848 | ACC: 1.0\n",
      "EPOCHS: 300 | LOSS: 0.022441613929368276 | ACC: 1.0\n",
      "EPOCHS: 301 | LOSS: 0.022321970521546858 | ACC: 1.0\n",
      "EPOCHS: 302 | LOSS: 0.022203459328672685 | ACC: 1.0\n",
      "EPOCHS: 303 | LOSS: 0.022086065688411045 | ACC: 1.0\n",
      "EPOCHS: 304 | LOSS: 0.021969775168044516 | ACC: 1.0\n",
      "EPOCHS: 305 | LOSS: 0.02185457356041573 | ACC: 1.0\n",
      "EPOCHS: 306 | LOSS: 0.021740446879948038 | ACC: 1.0\n",
      "EPOCHS: 307 | LOSS: 0.021627381358741163 | ACC: 1.0\n",
      "EPOCHS: 308 | LOSS: 0.021515363442741516 | ACC: 1.0\n",
      "EPOCHS: 309 | LOSS: 0.021404379787984117 | ACC: 1.0\n",
      "EPOCHS: 310 | LOSS: 0.021294417256906516 | ACC: 1.0\n",
      "EPOCHS: 311 | LOSS: 0.021185462914731966 | ACC: 1.0\n",
      "EPOCHS: 312 | LOSS: 0.021077504025920808 | ACC: 1.0\n",
      "EPOCHS: 313 | LOSS: 0.02097052805068908 | ACC: 1.0\n",
      "EPOCHS: 314 | LOSS: 0.020864522641592595 | ACC: 1.0\n",
      "EPOCHS: 315 | LOSS: 0.020759475640175197 | ACC: 1.0\n",
      "EPOCHS: 316 | LOSS: 0.020655375073680447 | ACC: 1.0\n",
      "EPOCHS: 317 | LOSS: 0.020552209151824666 | ACC: 1.0\n",
      "EPOCHS: 318 | LOSS: 0.020449966263630912 | ACC: 1.0\n",
      "EPOCHS: 319 | LOSS: 0.02034863497432223 | ACC: 1.0\n",
      "EPOCHS: 320 | LOSS: 0.020248204022273064 | ACC: 1.0\n",
      "EPOCHS: 321 | LOSS: 0.0201486623160178 | ACC: 1.0\n",
      "EPOCHS: 322 | LOSS: 0.020049998931315458 | ACC: 1.0\n",
      "EPOCHS: 323 | LOSS: 0.01995220310826894 | ACC: 1.0\n",
      "EPOCHS: 324 | LOSS: 0.019855264248498355 | ACC: 1.0\n",
      "EPOCHS: 325 | LOSS: 0.019759171912366952 | ACC: 1.0\n",
      "EPOCHS: 326 | LOSS: 0.01966391581625874 | ACC: 1.0\n",
      "EPOCHS: 327 | LOSS: 0.019569485829907064 | ACC: 1.0\n",
      "EPOCHS: 328 | LOSS: 0.01947587197377252 | ACC: 1.0\n",
      "EPOCHS: 329 | LOSS: 0.019383064416470086 | ACC: 1.0\n",
      "EPOCHS: 330 | LOSS: 0.019291053472243542 | ACC: 1.0\n",
      "EPOCHS: 331 | LOSS: 0.019199829598487237 | ACC: 1.0\n",
      "EPOCHS: 332 | LOSS: 0.019109383393313542 | ACC: 1.0\n",
      "EPOCHS: 333 | LOSS: 0.019019705593165463 | ACC: 1.0\n",
      "EPOCHS: 334 | LOSS: 0.018930787070473507 | ACC: 1.0\n",
      "EPOCHS: 335 | LOSS: 0.018842618831355646 | ACC: 1.0\n",
      "EPOCHS: 336 | LOSS: 0.01875519201336025 | ACC: 1.0\n",
      "EPOCHS: 337 | LOSS: 0.018668497883250198 | ACC: 1.0\n",
      "EPOCHS: 338 | LOSS: 0.018582527834827983 | ACC: 1.0\n",
      "EPOCHS: 339 | LOSS: 0.01849727338680134 | ACC: 1.0\n",
      "EPOCHS: 340 | LOSS: 0.01841272618068742 | ACC: 1.0\n",
      "EPOCHS: 341 | LOSS: 0.01832887797875607 | ACC: 1.0\n",
      "EPOCHS: 342 | LOSS: 0.01824572066201085 | ACC: 1.0\n",
      "EPOCHS: 343 | LOSS: 0.018163246228206666 | ACC: 1.0\n",
      "EPOCHS: 344 | LOSS: 0.018081446789904435 | ACC: 1.0\n",
      "EPOCHS: 345 | LOSS: 0.018000314572560988 | ACC: 1.0\n",
      "EPOCHS: 346 | LOSS: 0.01791984191265363 | ACC: 1.0\n",
      "EPOCHS: 347 | LOSS: 0.017840021255839774 | ACC: 1.0\n",
      "EPOCHS: 348 | LOSS: 0.01776084515514961 | ACC: 1.0\n",
      "EPOCHS: 349 | LOSS: 0.017682306269211738 | ACC: 1.0\n",
      "EPOCHS: 350 | LOSS: 0.017604397360511597 | ACC: 1.0\n",
      "EPOCHS: 351 | LOSS: 0.017527111293680925 | ACC: 1.0\n",
      "EPOCHS: 352 | LOSS: 0.017450441033819073 | ACC: 1.0\n",
      "EPOCHS: 353 | LOSS: 0.017374379644844253 | ACC: 1.0\n",
      "EPOCHS: 354 | LOSS: 0.01729892028787486 | ACC: 1.0\n",
      "EPOCHS: 355 | LOSS: 0.0172240562196403 | ACC: 1.0\n",
      "EPOCHS: 356 | LOSS: 0.01714978079092032 | ACC: 1.0\n",
      "EPOCHS: 357 | LOSS: 0.017076087445012916 | ACC: 1.0\n",
      "EPOCHS: 358 | LOSS: 0.01700296971622946 | ACC: 1.0\n",
      "EPOCHS: 359 | LOSS: 0.01693042122841734 | ACC: 1.0\n",
      "EPOCHS: 360 | LOSS: 0.016858435693509193 | ACC: 1.0\n",
      "EPOCHS: 361 | LOSS: 0.016787006910098032 | ACC: 1.0\n",
      "EPOCHS: 362 | LOSS: 0.016716128762038245 | ACC: 1.0\n",
      "EPOCHS: 363 | LOSS: 0.016645795217071428 | ACC: 1.0\n",
      "EPOCHS: 364 | LOSS: 0.01657600032547726 | ACC: 1.0\n",
      "EPOCHS: 365 | LOSS: 0.016506738218748075 | ACC: 1.0\n",
      "EPOCHS: 366 | LOSS: 0.016438003108287656 | ACC: 1.0\n",
      "EPOCHS: 367 | LOSS: 0.01636978928413276 | ACC: 1.0\n",
      "EPOCHS: 368 | LOSS: 0.016302091113697886 | ACC: 1.0\n",
      "EPOCHS: 369 | LOSS: 0.01623490304054206 | ACC: 1.0\n",
      "EPOCHS: 370 | LOSS: 0.016168219583157895 | ACC: 1.0\n",
      "EPOCHS: 371 | LOSS: 0.016102035333782006 | ACC: 1.0\n",
      "EPOCHS: 372 | LOSS: 0.016036344957226466 | ACC: 1.0\n",
      "EPOCHS: 373 | LOSS: 0.015971143189731044 | ACC: 1.0\n",
      "EPOCHS: 374 | LOSS: 0.015906424837835983 | ACC: 1.0\n",
      "EPOCHS: 375 | LOSS: 0.015842184777274296 | ACC: 1.0\n",
      "EPOCHS: 376 | LOSS: 0.015778417951883873 | ACC: 1.0\n",
      "EPOCHS: 377 | LOSS: 0.015715119372538847 | ACC: 1.0\n",
      "EPOCHS: 378 | LOSS: 0.015652284116099507 | ACC: 1.0\n",
      "EPOCHS: 379 | LOSS: 0.015589907324380866 | ACC: 1.0\n",
      "EPOCHS: 380 | LOSS: 0.015527984203139264 | ACC: 1.0\n",
      "EPOCHS: 381 | LOSS: 0.015466510021076844 | ACC: 1.0\n",
      "EPOCHS: 382 | LOSS: 0.015405480108863286 | ACC: 1.0\n",
      "EPOCHS: 383 | LOSS: 0.015344889858174629 | ACC: 1.0\n",
      "EPOCHS: 384 | LOSS: 0.01528473472074926 | ACC: 1.0\n",
      "EPOCHS: 385 | LOSS: 0.015225010207459692 | ACC: 1.0\n",
      "EPOCHS: 386 | LOSS: 0.015165711887401108 | ACC: 1.0\n",
      "EPOCHS: 387 | LOSS: 0.015106835386995513 | ACC: 1.0\n",
      "EPOCHS: 388 | LOSS: 0.015048376389111173 | ACC: 1.0\n",
      "EPOCHS: 389 | LOSS: 0.014990330632197894 | ACC: 1.0\n",
      "EPOCHS: 390 | LOSS: 0.014932693909436431 | ACC: 1.0\n",
      "EPOCHS: 391 | LOSS: 0.014875462067903758 | ACC: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 392 | LOSS: 0.014818631007751255 | ACC: 1.0\n",
      "EPOCHS: 393 | LOSS: 0.014762196681398443 | ACC: 1.0\n",
      "EPOCHS: 394 | LOSS: 0.014706155092739778 | ACC: 1.0\n",
      "EPOCHS: 395 | LOSS: 0.014650502296365126 | ACC: 1.0\n",
      "EPOCHS: 396 | LOSS: 0.014595234396793827 | ACC: 1.0\n",
      "EPOCHS: 397 | LOSS: 0.014540347547721863 | ACC: 1.0\n",
      "EPOCHS: 398 | LOSS: 0.014485837951281513 | ACC: 1.0\n",
      "EPOCHS: 399 | LOSS: 0.014431701857314218 | ACC: 1.0\n",
      "EPOCHS: 400 | LOSS: 0.014377935562655219 | ACC: 1.0\n",
      "EPOCHS: 401 | LOSS: 0.01432453541043097 | ACC: 1.0\n",
      "EPOCHS: 402 | LOSS: 0.01427149778936775 | ACC: 1.0\n",
      "EPOCHS: 403 | LOSS: 0.014218819133112806 | ACC: 1.0\n",
      "EPOCHS: 404 | LOSS: 0.0141664959195663 | ACC: 1.0\n",
      "EPOCHS: 405 | LOSS: 0.014114524670224842 | ACC: 1.0\n",
      "EPOCHS: 406 | LOSS: 0.014062901949536199 | ACC: 1.0\n",
      "EPOCHS: 407 | LOSS: 0.014011624364264598 | ACC: 1.0\n",
      "EPOCHS: 408 | LOSS: 0.01396068856286679 | ACC: 1.0\n",
      "EPOCHS: 409 | LOSS: 0.013910091234878706 | ACC: 1.0\n",
      "EPOCHS: 410 | LOSS: 0.013859829110312305 | ACC: 1.0\n",
      "EPOCHS: 411 | LOSS: 0.013809898959062301 | ACC: 1.0\n",
      "EPOCHS: 412 | LOSS: 0.013760297590323317 | ACC: 1.0\n",
      "EPOCHS: 413 | LOSS: 0.013711021852016045 | ACC: 1.0\n",
      "EPOCHS: 414 | LOSS: 0.013662068630223576 | ACC: 1.0\n",
      "EPOCHS: 415 | LOSS: 0.013613434848636762 | ACC: 1.0\n",
      "EPOCHS: 416 | LOSS: 0.013565117468008796 | ACC: 1.0\n",
      "EPOCHS: 417 | LOSS: 0.013517113485618988 | ACC: 1.0\n",
      "EPOCHS: 418 | LOSS: 0.013469419934745196 | ACC: 1.0\n",
      "EPOCHS: 419 | LOSS: 0.013422033884144924 | ACC: 1.0\n",
      "EPOCHS: 420 | LOSS: 0.01337495243754544 | ACC: 1.0\n",
      "EPOCHS: 421 | LOSS: 0.013328172733141512 | ACC: 1.0\n",
      "EPOCHS: 422 | LOSS: 0.013281691943101966 | ACC: 1.0\n",
      "EPOCHS: 423 | LOSS: 0.013235507273084283 | ACC: 1.0\n",
      "EPOCHS: 424 | LOSS: 0.013189615961756676 | ACC: 1.0\n",
      "EPOCHS: 425 | LOSS: 0.013144015280328466 | ACC: 1.0\n",
      "EPOCHS: 426 | LOSS: 0.013098702532087994 | ACC: 1.0\n",
      "EPOCHS: 427 | LOSS: 0.01305367505194775 | ACC: 1.0\n",
      "EPOCHS: 428 | LOSS: 0.013008930205997407 | ACC: 1.0\n",
      "EPOCHS: 429 | LOSS: 0.012964465391063495 | ACC: 1.0\n",
      "EPOCHS: 430 | LOSS: 0.012920278034276709 | ACC: 1.0\n",
      "EPOCHS: 431 | LOSS: 0.012876365592645897 | ACC: 1.0\n",
      "EPOCHS: 432 | LOSS: 0.01283272555263907 | ACC: 1.0\n",
      "EPOCHS: 433 | LOSS: 0.012789355429771108 | ACC: 1.0\n",
      "EPOCHS: 434 | LOSS: 0.01274625276819789 | ACC: 1.0\n",
      "EPOCHS: 435 | LOSS: 0.012703415140317337 | ACC: 1.0\n",
      "EPOCHS: 436 | LOSS: 0.012660840146376593 | ACC: 1.0\n",
      "EPOCHS: 437 | LOSS: 0.012618525414085383 | ACC: 1.0\n",
      "EPOCHS: 438 | LOSS: 0.012576468598235593 | ACC: 1.0\n",
      "EPOCHS: 439 | LOSS: 0.01253466738032724 | ACC: 1.0\n",
      "EPOCHS: 440 | LOSS: 0.012493119468199934 | ACC: 1.0\n",
      "EPOCHS: 441 | LOSS: 0.012451822595670115 | ACC: 1.0\n",
      "EPOCHS: 442 | LOSS: 0.012410774522174808 | ACC: 1.0\n",
      "EPOCHS: 443 | LOSS: 0.012369973032419967 | ACC: 1.0\n",
      "EPOCHS: 444 | LOSS: 0.012329415936035202 | ACC: 1.0\n",
      "EPOCHS: 445 | LOSS: 0.012289101067233393 | ACC: 1.0\n",
      "EPOCHS: 446 | LOSS: 0.01224902628447584 | ACC: 1.0\n",
      "EPOCHS: 447 | LOSS: 0.012209189470143031 | ACC: 1.0\n",
      "EPOCHS: 448 | LOSS: 0.01216958853020979 | ACC: 1.0\n",
      "EPOCHS: 449 | LOSS: 0.012130221393926069 | ACC: 1.0\n",
      "EPOCHS: 450 | LOSS: 0.012091086013502755 | ACC: 1.0\n",
      "EPOCHS: 451 | LOSS: 0.012052180363801972 | ACC: 1.0\n",
      "EPOCHS: 452 | LOSS: 0.012013502442032524 | ACC: 1.0\n",
      "EPOCHS: 453 | LOSS: 0.011975050267449862 | ACC: 1.0\n",
      "EPOCHS: 454 | LOSS: 0.011936821881060952 | ACC: 1.0\n",
      "EPOCHS: 455 | LOSS: 0.011898815345333363 | ACC: 1.0\n",
      "EPOCHS: 456 | LOSS: 0.01186102874390908 | ACC: 1.0\n",
      "EPOCHS: 457 | LOSS: 0.011823460181322664 | ACC: 1.0\n",
      "EPOCHS: 458 | LOSS: 0.011786107782723843 | ACC: 1.0\n",
      "EPOCHS: 459 | LOSS: 0.011748969693603997 | ACC: 1.0\n",
      "EPOCHS: 460 | LOSS: 0.01171204407952737 | ACC: 1.0\n",
      "EPOCHS: 461 | LOSS: 0.011675329125865856 | ACC: 1.0\n",
      "EPOCHS: 462 | LOSS: 0.011638823037538394 | ACC: 1.0\n",
      "EPOCHS: 463 | LOSS: 0.011602524038753687 | ACC: 1.0\n",
      "EPOCHS: 464 | LOSS: 0.011566430372757347 | ACC: 1.0\n",
      "EPOCHS: 465 | LOSS: 0.01153054030158268 | ACC: 1.0\n",
      "EPOCHS: 466 | LOSS: 0.011494852105805272 | ACC: 1.0\n",
      "EPOCHS: 467 | LOSS: 0.011459364084301184 | ACC: 1.0\n",
      "EPOCHS: 468 | LOSS: 0.011424074554008932 | ACC: 1.0\n",
      "EPOCHS: 469 | LOSS: 0.011388981849695107 | ACC: 1.0\n",
      "EPOCHS: 470 | LOSS: 0.011354084323723165 | ACC: 1.0\n",
      "EPOCHS: 471 | LOSS: 0.011319380345826262 | ACC: 1.0\n",
      "EPOCHS: 472 | LOSS: 0.011284868302882837 | ACC: 1.0\n",
      "EPOCHS: 473 | LOSS: 0.011250546598696147 | ACC: 1.0\n",
      "EPOCHS: 474 | LOSS: 0.01121641365377657 | ACC: 1.0\n",
      "EPOCHS: 475 | LOSS: 0.011182467905127714 | ACC: 1.0\n",
      "EPOCHS: 476 | LOSS: 0.011148707806035097 | ACC: 1.0\n",
      "EPOCHS: 477 | LOSS: 0.011115131825858344 | ACC: 1.0\n",
      "EPOCHS: 478 | LOSS: 0.011081738449826586 | ACC: 1.0\n",
      "EPOCHS: 479 | LOSS: 0.011048526178836407 | ACC: 1.0\n",
      "EPOCHS: 480 | LOSS: 0.01101549352925307 | ACC: 1.0\n",
      "EPOCHS: 481 | LOSS: 0.010982639032714982 | ACC: 1.0\n",
      "EPOCHS: 482 | LOSS: 0.0109499612359402 | ACC: 1.0\n",
      "EPOCHS: 483 | LOSS: 0.010917458700536689 | ACC: 1.0\n",
      "EPOCHS: 484 | LOSS: 0.010885130002814616 | ACC: 1.0\n",
      "EPOCHS: 485 | LOSS: 0.01085297373360217 | ACC: 1.0\n",
      "EPOCHS: 486 | LOSS: 0.010820988498063032 | ACC: 1.0\n",
      "EPOCHS: 487 | LOSS: 0.010789172915517684 | ACC: 1.0\n",
      "EPOCHS: 488 | LOSS: 0.010757525619266347 | ACC: 1.0\n",
      "EPOCHS: 489 | LOSS: 0.010726045256415028 | ACC: 1.0\n",
      "EPOCHS: 490 | LOSS: 0.010694730487703905 | ACC: 1.0\n",
      "EPOCHS: 491 | LOSS: 0.010663579987338257 | ACC: 1.0\n",
      "EPOCHS: 492 | LOSS: 0.010632592442821722 | ACC: 1.0\n",
      "EPOCHS: 493 | LOSS: 0.01060176655479221 | ACC: 1.0\n",
      "EPOCHS: 494 | LOSS: 0.010571101036859813 | ACC: 1.0\n",
      "EPOCHS: 495 | LOSS: 0.01054059461544753 | ACC: 1.0\n",
      "EPOCHS: 496 | LOSS: 0.010510246029633647 | ACC: 1.0\n",
      "EPOCHS: 497 | LOSS: 0.010480054030996907 | ACC: 1.0\n",
      "EPOCHS: 498 | LOSS: 0.010450017383463867 | ACC: 1.0\n",
      "EPOCHS: 499 | LOSS: 0.010420134863157985 | ACC: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = NNBP(epochs=500)\n",
    "loss_lst2, acc_lst2 = model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b20a89ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(loss_lst1, label='standard')\n",
    "plt.plot(loss_lst2, label='cumulative')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(acc_lst1, label='standard')\n",
    "plt.plot(acc_lst2, label='cumulative')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
